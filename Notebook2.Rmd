---
title: "Notebook"
output: html_document
date: "2024-11-09"
---

**SET UP**

```{r}
rm(list = ls())

# Importing libraries
library("dplyr")
library("skimr")
library("ggplot2")
library("ggrepel")
library("stringi")
library("GGally")
library("factoextra")
library("corrplot")
library("FactoMineR")
```

We have different datasets which contain different statistics from different players which we will need to combine to obtain a richer and more useful dataset.

```{r}
# Reading the different datasets
defense_stats <- read.csv("Big5-defenseStats22-23.csv", sep = ",")
gca_stats <- read.csv("Big5-gcaStats22-23.csv", sep = ",")
misc_stats <- read.csv("Big5-miscStats22-23.csv", sep = ",")
passing_stats <- read.csv("Big5-passingStats22-23.csv", sep = ",")
possession_stats <- read.csv("Big5-possessionStats22-23.csv", sep = ",")
shooting_stats <- read.csv("Big5-shootingStats22-23.csv", sep = ",")

# Datasets for goalkeepers
keepers_one <- read.csv("Big5-keepersStats22-23.csv", sep = ",")

# Dataset for all players combined
playingtime <- read.csv("Big5-playingtimeStats22-23.csv", sep = ",")

```

**DATA PREPROCESSING**

Merging all datasets in one when each of them has a lot of datasets will potentially cause many problems in the long run, we won't handle duplicates or NA values just yet and fully focus on reducing the amount of dimensions each dataset has.

Therefore, we will look at each dataset independently handling the unimportant columns or columns that can be inferred from other datasets to reduce dimensionality. We will also rename the variables remaining for a better understanding.

```{r}
# Defense Stats

# The total amount of tackles is the sum of Def.3rd, Mid.3rd and Att.3rd which just refer to where those tackles were made, Tkl.Int is a sum of both tackles and interceptions therefore we can keep this columns and eliminate both interceptions and tackles. Blocks are a sum of Sh and Pass which refers to the different type of blocks. Tkl.1 is tackles in the first half, already included in the first variable. Tkl.1 measures the percentage of lost balls when Att is done, it divides Att and Lost. Many players dont have errors, and at the same time errors leading to goal are mostly done by defenders as an error in the defense side is more crucial, therefore we will eliminate it as well.

defense_cols <- c("Def.3rd","Mid.3rd","Att.3rd","Tkl.1","Int","Sh", "Pass", "Att", "Lost","Err")

defense_stats <- defense_stats %>% select(-all_of(defense_cols))

defense_stats <- defense_stats %>% 
  rename(TacklesDone = Tkl, SuccessfulTackles = TklW, TacklesANDIntercep = Tkl.Int,
         Clearances = Clr, PercDribblersTackled = Tkl.)


# GCA stats

# SCA is an advanced metric that tracks the two offensive/attacking actions that directly lead to a shot on # goal, Following the same logic as a Shot-Creating Action, a Goal-Creating Action is an advanced metric that tracks the two offensive/attacking actions that leads to a goal. These follow the same criteria as  the above list. The difference is that one causes a goald, the other an scoring chance

# SCA is a sum of PassLive, PassDead, TO, Sh Fld, Def, they are different stats that contribute to it, and  SCA90s can be inferred as SCA*90s returns SCA90s
# GCA follows the same deduction as before

gca_cols <- c("SCA90","PassLive", "PassDead", "TO", "Sh", "Fld", "Def", "GCA90", "PassLive.1", "PassDead.1", "TO.1", "Sh.1", "Fld.1", "Def.1")

gca_stats <- gca_stats %>% select(-all_of(gca_cols))

gca_stats <- gca_stats %>% 
  rename(ShotCreatingActions = SCA, GoalCreatingActions = GCA )

# Misc stats

# X2CrdY is when the player is suspended by 2 yellow cards instead of one red card, this is included in the Red Card statistic. TklW and Int are already included in the defense dataset, Won. refers to the aerial duels won, and it divides won and lost, so we can remove them. Own goals are many times fortunate and dont affect the skill value of a player. PKwon and PKcon refer to the amount of penalties conceded and won, but not many players have a value here so we will remove it as it is irrelevant. Crs refers to crosses and will appear in a future dataset 

misc_cols <- c("X2CrdY", "TklW", "Int", "Won", "Lost", "OG", "PKwon", "PKcon", "Crs")


misc_stats <- misc_stats %>% select(-all_of(misc_cols))

misc_stats <- misc_stats %>% 
  rename(YellowCards = CrdY, RedCards = CrdR, FoulsCommited = Fls, FoulsDrawn = Fld, Offsides = Off, RecoveredBalls = Recov, PercAerialDuelsWon = Won.)

# Passing Stats

#Cmp. is the percentage of the completed passes in terms of the attempted ones, therefore we can remove them. Cmp.1, Att.1, Cmp..1,Cmp.2,Att.2, Cmp..2, Cmp.3,Att.3,Cmp..3 are all relative to the distance of the passes, which is not of much interest as of right now and both TotDist and PrgDist contain a lot of information about the distance of passes. 1/3, PPA, CrsPa, PrgP are also covered as they all refer to progressive passes. A.xAG is assists minus the xAG, therefore we can remove it.

passing_cols <- c("Cmp.","Cmp.1", "Att.1", "Cmp..1","Cmp.2","Att.2", "Cmp..2", "Cmp.3","Att.3","Cmp..3","X1.3", "PPA", "CrsPA", "PrgP","A.xAG")

passing_stats <- passing_stats %>% select(-all_of(passing_cols))

passing_stats <- passing_stats %>% 
  rename(CompletedPasses = Cmp, AttemptedPasses = Att, PassingDistance = TotDist, ForwardPassingDistance = PrgDist, Assists = Ast, ExpectedAssistsGoal = xAG, ExpectedAssists = xA,  KeyPasses = KP)

# Posession Stats


# Touches is a sum of Def.Pen, Def.3rd, Mid.3rd, Att.3rd, Att.Pen, Live so we can remove them. Succ. is the division of Att and Succ which refers to the number of times the attacker has dribbled the defender divided by the total attemps. Tkld, Tkld. are two variables directly related to the previous so we can also take them out. X1.3, CPA, Mis are not really valuable. TotDist and PrgDist will have different names to differentiate from the previous dataset that used the same variables. Rec, PrgR refer to the times the player received a good pass where in most of the cases if a pass is missed it is usually the passers fault rather than the receiver.


posession_cols <- c("Def.Pen", "Def.3rd", "Mid.3rd", "Att.3rd", "Att.Pen", "Live", "Att", "Succ", "Tkld", "Tkld.","X1.3", "CPA", "Mis", "Rec", "PrgR", "PrgC")

possession_stats <- possession_stats %>% select(-all_of(posession_cols))

possession_stats <- possession_stats %>% 
  rename(DistanceRanBall = TotDist, ForwardDistanceRanBall = PrgDist, PercSuccessfulDribbles = Succ., Dispossessed = Dis)

# Shooting Stats

# With goals, shots and shots on target we can infer SoT., Sh.90, SoT.90, G.Sh, G.SoT. As not many players shoot penalties or free kicks we will not take into account this, FK, PK and PKatt will be removed. npxG/Sh can be calculated with both variables, as well as G.xG and np.G.xG can also be calculated with npxG and xG

shooting_cols <- c("SoT.", "Sh.90", "SoT.90", "G.Sh", "G.SoT","FK", "PK","PKatt", "npxG.Sh", "G.xG", "np.G.xG")

shooting_stats <- shooting_stats %>% select(-all_of(shooting_cols))

shooting_stats <- shooting_stats %>% 
  rename(Goals = Gls, Shots = Sh, ShotsOnTarget = SoT, ShotDistance = Dist, ExpectedGoals = xG, NonPenaltyExpectedGoals = npxG)

# Keepers one

# The statistics MP, Starts, Min will be in another dataset so we will remove them. Save. is calculated by SoTA-GA/SoTa and CS. which refers to clean sheets can also be removed. Everything about free kicks and penalties can be removed as it is easy to score from a penalty and harder from a free kick and it usually depends on the quality of the attacker, many times the keeper cannot do much.We will remove PKatt, PKA, PKsv,PKm,Save..1. GA90 can also be inffered with Goals Against and 90s

keepers_cols <- c("MP", "Starts", "Min", "Save.", "CS.", "PKatt", "PKA", "PKsv","PKm","Save..1", "GA90")

keepers_one <- keepers_one %>% select(-all_of(keepers_cols))

keepers_one <- keepers_one %>% 
  rename(GoalsAllowed = GA, ShotsOnTargetAgainst = SoTA, Won = W, Drawn = D, Lost = L, CleanSheets = CS)



# Playing Time

# Min can be calcualted by 90 * X90S.Mn.Mp can be calculated as Min/Mp, Mn. can also be calculated. Mn.Start will be Min/Starts, Compl can be seen as it is not really important. The same for all sub variables as with minutes we can already kind of see the amount of time played. We will remove Subs, Mn.Sub, unSub. X... is substraction of onG and onGA so we can remove it, and the same for the 90s, X...90. Same for On.Off.1 and On.Off. Minutes is a character therefore we will have to change to numeric

playing_cols <- c("Mn.MP","Min.","Min", "Mn.Start", "Compl", "Subs", "Mn.Sub", "unSub","X...", "X...90", "On.Off", "On.Off.1", "xG...", "xG...90")

playingtime <- playingtime %>% select(-all_of(playing_cols))

playingtime <- playingtime %>% 
  rename(MatchesPlayed = MP, PointsPerMatch = PPM, GoalsByTeam = onG, GoalsAgainstTeam = onGA, ExepectedGoalsByTeam = onxG, ExpectedGoalsAgainstTeam = onxGA )





```

Merging all datasets according to different columns and dealing with missing values and duplicates later, this can be done with the inner join function taken into account that all datasets have the same 8 first columns.

```{r}
# Making a list with the repeated columns
repeated_columns = c("Player", "Nation", "Pos", "Squad", "Comp", "Age", "Born", "X90s")


# Inner joining all datasets based on key columns and a brief look at it
merged_players <- inner_join(defense_stats, gca_stats, by = repeated_columns)%>%
                  inner_join(misc_stats, by = repeated_columns)%>%
                  inner_join(passing_stats, by = repeated_columns)%>%
                  inner_join(possession_stats, by = repeated_columns)%>%
                  inner_join(shooting_stats, by = repeated_columns)

skim(merged_players)


```

We now have the problem that goalkeepers have different stats to players, therefore the number of variables varies and we cannot join them to the same dataset. Instead we need to add the columns which are not in players to goalkeepers with a 0 value for each entry and viceversa.

```{r}
# Obtaining all columns in players and goalkeepers
player_columns <- colnames(merged_players)
gk_columns <- colnames(keepers_one)

# Finding out the different columns in each of the datasets
missing_columns_players = setdiff(gk_columns, player_columns)
missing_columns_gk = setdiff(player_columns, gk_columns)

# Using a for loop to create the missing columns with a value of 0
for (col in missing_columns_players) {
  merged_players[[col]] <- 0
}

for (col in missing_columns_gk) {
  keepers_one[[col]] <- 0
}

# Combining both goalkeepers and players
all_players = bind_rows(merged_players,keepers_one)

# Merging with playing time stats
final_players <- inner_join(all_players, playingtime, by = repeated_columns)
head(final_players)

```

We observe many redundancies in this new dataset, such as that the nations are repeated in both lower and upper case, that some players have more than one position or that the competition also has lower case in front which could be troubling in the future. We will remove the strings with gsub and substr, and maintain only the first position for all players.

```{r}
# Removing lower case in nation
final_players$Nation <- gsub("[a-z]", "", final_players$Nation)
# Saving only the first position
final_players$Pos <- substr(final_players$Pos,1,2)
# Removing lower case before leagues
final_players$Comp <- substr(final_players$Comp, 4, nchar(final_players$Comp))


# With that solved we will look at the players now to check for NAs and duplicated values
head(final_players)
```

Apart from that we also see there are a total of +200 duplicates which could be from players who were transferred or were loaned mid-season. To deal with this, we will keep the entries with the most time played.

```{r}
# Removing duplicate players
final_players <- final_players %>%
  group_by(Player) %>%          
  # Keep the observation with the most time played
  slice_max(order_by = X90s, n = 1) %>%    
  ungroup()

# Check that there are no duplicates now
sum(duplicated(final_players))
```

**NA Values**

Using a barplot to see the columns with NA values.

```{r}
barplot(colMeans(is.na(final_players)), las=2)
```

**Median Imputation**

Since all NA values are in numerical columns, we can replace these NAs by the median of that column.

```{r}
# Replace NA values with the median of each column
final_players <- final_players %>%
  mutate(across(everything(), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))

# Number of rows with NA values 
nrow(final_players %>% filter(if_any(everything(), is.na)))
```

We see that this dataset has many statistics for players, but we could also benefit from other ones that are not there, such as the overall player rating or their price. For this we will merge part of another dataset which bases its rating and some other statistics in the FIFA videogame.

```{r}
# Reading complimentary data and removing unnecessary columns
data_fifa <- read.csv("CLEAN_FIFA23_official_data.csv")
head(data_fifa)
colnames(data_fifa)

# Checking for duplicate values
sum(duplicated(data_fifa$Name))
```

As it has happened with our previous football datasets, we will have to modify duplicate values

```{r}
# Obtaining the data without repeated players
data_fifa <- data_fifa %>%
  group_by(Name) %>%
  # Keeping only observations with the most recent joined date
  slice_max(Joined, n = 1, with_ties = FALSE) %>%  
  ungroup()

# Check that there are no duplicates now
sum(duplicated(final_players))


```

We can also see that the dataset is already clean with no NA values but looking at the different columns and variables there are some columns which do not provide much information, are already in the previous dataset or are not of our interest for our current project, therefore, we will remove them. Since the merging will be done by name it is the only column present in other datasets that we will maintain. The names of the players in the fifa dataset has numbers before which can lead to errors when merging as well, so we will have to remove it.

```{r}
colnames(data_fifa)

cols <- c("X", "ID", "Age", "Photo","Flag", "Club", "Club.Logo", "Special","Work.Rate","Skill.Moves", "Weak.Foot","Body.Type", "Real.Face", "Position", "Joined", "Loaned.From", "Contract.Valid.Until","Release.Clause...", "Kit.Number", "Best.Overall.Rating", "Year_Joined", "Potential", "Preferred.Foot", "International.Reputation", "Nationality")

data_fifa <- data_fifa %>% select(-all_of(cols))

data_fifa$Name <- gsub("[0-9]+", "", data_fifa$Name)

```

Now that we have cleaned a bit the other dataset we can proceed to merging it with this new and last one.

For this there is a problem, where FIFA players have different name formats ("Messi", "L. Messi" or "Lionel Messi"). To solve this we will first standardized name to the format "L. Messi" and then extract the surnames for both datasets and join by means of the surname.

Also we will replace before all of this all characters from other languages into the closest one in the English alphabet ("è" -\> "e")

```{r}
# Replacing characters by its closest English alphabet character
final_players$Player <- stri_trans_general(final_players$Player, "latin-ascii")
data_fifa$Name <- stri_trans_general(data_fifa$Name, "latin-ascii")

# Handle additional characters manually for specific languages 
final_players$Player <- gsub("ć|č", "c", final_players$Player)
final_players$Player <- gsub("ł", "l", final_players$Player)
final_players$Player <- gsub("ń", "n", final_players$Player)
final_players$Player <- gsub("ă", "a", final_players$Player)
final_players$Player <- gsub("ș|ş", "s", final_players$Player)
final_players$Player <- gsub("ž|ź", "z", final_players$Player)

# Removes other symbols
final_players$Player <- gsub("[^[:alnum:] ]", "", final_players$Player)  

data_fifa$Name <- gsub("ć|č", "c", data_fifa$Name)
data_fifa$Name <- gsub("ł", "l", data_fifa$Name)
data_fifa$Name <- gsub("ń", "n", data_fifa$Name)
data_fifa$Name <- gsub("ă", "a", data_fifa$Name)
data_fifa$Name <- gsub("ș|ş", "s", data_fifa$Name)
data_fifa$Name <- gsub("ž|ź", "z", data_fifa$Name)
data_fifa$Name <- gsub("[^[:alnum:] ]", "", data_fifa$Name)



# Set a Name column to the format "L. Messi"
data_fifa$StandardizedName <- paste0(substr(data_fifa$Name, 1, 1), ". ", sapply(strsplit(data_fifa$Name, " "), function(x) x[length(x)]))

# Create a new column of the surname
final_players$Surname <- sapply(strsplit(final_players$Player, " "), function(x) x[length(x)])
data_fifa$Surname <- sapply(strsplit(data_fifa$StandardizedName, " "), function(x) x[length(x)])

# Merge both datasets by Surname
full_data <- inner_join(final_players, data_fifa, by = "Surname")

# Eliminating repeated values when merging has not been correct
full_data <- na.omit(full_data)
full_data <- full_data %>%
  group_by(Player) %>%          
  # Keep the observation with the most time played
  slice_max(order_by = GoalsAllowed, n = 1, with_ties = FALSE) %>%    
  ungroup()

names = full_data[,1]
```

We observe that some observations have been lost. If we look closely at the data most of those observations not merged do not appear in the FIFA dataset so, since they are not very relevant we can exclude them confidently.

```{r}
# Observations not merged
not_merged <- anti_join(final_players, full_data, by = "Player")
```

**Outliers**

Outliers are actually important as many players could have a fantastic season compared to others so it is important to keep them. But there are many players who are not in the managers rotation and they do not play much minutes so we will eliminate them as they will not provide useful information.

```{r}
ggplot(full_data) +
  aes(x = X90s)+ 
  geom_area(stat = "bin",fill="lightblue", color="blue", binwidth = 40) + 
   theme_minimal()
```

X90s is a representation of the total minutes played divided by 90, which shows more or less the amount of games played. Most leagues have a total of 38 games of 90 minutes, with the added minutes, it makes sense the maximum value is 40. But what is really useful here is that there are many players who have not even played a single game and therefore will be irrelevant in our project and analysis, so we will remove them.

```{r}

# Eliminating players that have not played much
full_data <- full_data[full_data$X90s > 2,]
```

**Exploratory Data Analysis**

We will now create some graphs to generate some insights and obtain information about the players.

Focusing now on goals and assists, as they are one of the most important variables as goals and assists make teams win games, we create this graph.

In this plot, we can see as expected that forwards have higher values, it is also remarkable that we have some goalkeepers with an assist or goal. Players above the line have scored less goals than expected while players further to the right and below the line have scored more goals than it was expected. Therefore it is desirable to be on the bottom right of the plot for great goals and assists conversion.

Also we can clearly see that there is a linear relationship between both variables.

```{r}
plot1 <- ggplot(full_data)+
  aes(x = Assists+Goals, y = ExpectedGoals+ExpectedAssists, color = Pos)+
  geom_point(alpha=0.85)+
  geom_text_repel(
    data = subset(full_data, Assists + Goals > 25), 
    aes(label = Player),
    size = 2
  ) +
  geom_abline()+
  ggtitle(" Goals and Assists vs Expected Goals and Assists")+
  xlab("Total Goals + Assists") +                      
  ylab("Expected Goals + Assists") + 
  theme(plot.title = element_text(hjust = 0.5))

# Save the plot 
#ggsave("goals_assists_plot.png", plot = plot, width = 10, height = 8, dpi = 300)

```

In this plot we focus on offensive players. Usually players that are more game-changing are targeted by opposing players, and fouling them is one of the most common strategies to stop them. This is why in this plot we compare fouls drawn and the success chance in dribbles.

In the plot we can see that players that are good dribblers are usually the most fouled, and also have many carries. The biggest example of this is Vinicius Jr.

```{r}
plot2 <- ggplot(full_data)+
  aes(x = PercSuccessfulDribbles, y = FoulsDrawn, color = Pos, size=Carries)+
  geom_point(alpha = 0.85)+
  scale_size_continuous(range = c(1, 8)) +
  geom_text_repel(
    data = subset(full_data, FoulsDrawn > 70), 
    aes(label = Player),
    size = 2
  ) +
  ggtitle("Successful Dribble % vs Fouls Drawn")+
  xlab("Successful Dribble %") +                      
  ylab("Fouls Drawn") + 
  theme(plot.title = element_text(hjust = 0.5))

# Save the plot 
#ggsave("dribbles_fouls_plot.png", plot = plot, width = 10, height = 8, dpi = 300)
```

In this plot we focus on passes. With this density graph, we observe the range where most players fall into, but also we observe that as players tend to pass more, their accuracy also increases.

```{r}
plot3 <- ggplot(full_data, aes(x=100*CompletedPasses/AttemptedPasses, y=PassingDistance/X90s)) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon", colour="white")+
  ggtitle("Pass Accuracy % vs Passing Distance per Game")+
  xlab("Pass Accuracy %") +                      
  ylab(" Avg. Passing Distance per Game") + 
  theme(plot.title = element_text(hjust = 0.5))

#ggsave("pass_perc_distance_plot.png", plot = plot, width = 10, height = 8, dpi = 300)
```

Next, we created a correlation plot for defensive actions. As we can see except Aerial Duels, the other ones are closely related. However, this defensive actions are more correlated in midfielders, and sometimes forwards, than in defenders which is surprising.

```{r}
# Select only the relevant columns for the correlogram
correlogram_data <- full_data[, c("SuccessfulTackles", "Blocks", "FoulsCommited", "RecoveredBalls", "PercAerialDuelsWon")]

# Create the correlogram
plot4 <- ggpairs(correlogram_data, 
        title = "Correlogram of Defensive Actions",
        lower = list(continuous = wrap("points", alpha = 0.3)),
        upper = list(continuous = wrap("cor", size = 4)), 
        ggplot2::aes(colour=full_data$Pos)) +
  theme(plot.title = element_text(hjust = 0.5))  # Center the title

#ggsave("defense_corr_plot.png", plot = plot, width = 10, height = 8, dpi = 300)
```

Here we study goalkeepers. As we can see as the save % increases, the clean sheets also increase but the relation is not that clear, also there are some clear outliers, such as MarcAndre ter Stegen.

```{r}
plot5 <- ggplot(full_data)+
  aes(x = 100*Saves/ShotsOnTargetAgainst, y = CleanSheets, color = Comp)+
  geom_point(alpha = 0.85)+
  geom_text_repel(
    data = subset(full_data, Saves/ShotsOnTargetAgainst > 0.6 & CleanSheets >= 10), 
    aes(label = Player),
    size = 2
  ) +
  ggtitle("Save % vs Clean Sheets")+
  xlab("Save %") +                      
  ylab("Clean Sheets") + 
  theme(plot.title = element_text(hjust = 0.5))

# Save the plot 
#ggsave("goalkeepers_plot.png", plot = plot, width = 10, height = 8, dpi = 300)
```

**Descriptive Analysis**

Before using any algorithm on the data we have to check whether or not we scale the data, and also select the numeric columns and the ones we are interested in. We will start by choosing some numeric columns and plotting them to see their distribution.

```{r}

numeric_columns <- full_data[, c("X90s", "TacklesDone", "SuccessfulTackles", "PercDribblersTackled", "Blocks", "TacklesANDIntercep", "Clearances", "ShotCreatingActions", "GoalCreatingActions","PointsPerMatch","GoalsByTeam", "GoalsAgainstTeam")] 

par(mfrow = c(3, 3)) # Adjust the layout for 4x4 grid to fit all histograms

for (col_name in names(numeric_columns)) {
  hist(
    numeric_columns[[col_name]], 
    main = paste("Histogram of", col_name), 
    xlab = col_name, 
    col = "skyblue", 
    border = "white"
  )
}

```

Most of these plots are left skewed, one reason for this is due to the number of goalkeepers, as these position has no value for these variables recorded. For goalkeeper stats it would happen exactly the same. Apart from that, most teams may have around 25 players, while the lineups are of 11 players, these leads to difficulties distributing the number of minutes between the players so the recorded stat for most players is low.
Now we will use all numeric variables and see the relations and information we can obtain.

```{r}
# Was used to show the different histograms, now we have to delete it
par(mfrow = c(1, 1))


# Need to select numeric data from the dataset
numeric_data <- select_if(full_data, is.numeric)

# Using boxplots to visualize a few columns
boxplot(numeric_columns, las=2, col="darkblue")

```

These boxplots clearly show that we will need to scale the data before performing any algorithm on them. This is due to the difference of variance between each variables.

As the data is mostly left skewed, to fix this we can apply a root to the numeric columns.

```{r}

# Scaling the data
scaled_data <- scale(sqrt(numeric_data)) #Numeric columns no numeric data

boxplot(scaled_data, las=2, col="darkblue")

```
The variables returned are now scaled, so there will be no big differences in variances. There are some very small boxplots in the middle, this is because this statistics refer to goalkeepers, where all players have a 0, and therefore the amount of outliers is big, but it actually corresponds to goalkeepers.


Let's look at the correlation between the different variables, to see if we can obtain some insights before running any algorithm. Using the corrplot library for a better understanding.

```{r}
correlation_matrix = cor(numeric_data)

corrplot(correlation_matrix, 
         method = "ellipse",       
         type = "lower",           
         order = "hclust",         
         tl.col = "black",         
         tl.srt = 45,              
         tl.cex = 0.5,             
         cl.cex = 0.5)

# eigen(correlation_matrix)
# This line here would return 57 eigenvalues, one per column, it explains the amount of variance explained by each component.
# It would also return the eigenvectors, which show the value of each variable in each component.

```

This shows the correlation for the different variables, some of these have high correlation which could affect the PCA. There are also some variables with no or very low correlation which is why the PCA could offer more information about the variables.


**PRINCIPAL COMPONENT ANALYSIS**

```{r}
#Scale is set to False as the data is already scaled and this could lead to errors
pca = prcomp(scaled_data, scale=F)

summary(pca)
get_eigenvalue(pca)

```

***Choosing the number of components***

```{r}
# Using factoextra package to see the variances explained by each component
fviz_screeplot(pca, addlabels = TRUE)
```

2 dimensions explain 59.7% of the variability while 3 dimensions explain 71.11%.


```{r}
corrplot(
  var$cos2, 
  is.corr = FALSE, 
  method = "color",               
  type = "lower",                 
  tl.cex = 0.5,                   
  tl.srt = 45,                    
  cl.cex = 0.5,
  tl.col = "black",
  title = "Representation of Variables by Each Component",
  mar = c(0, 0, 2, 0) 
)
```
This graph uses cos^2, which refers to the quality of representation of the different variables by each principal component. A high value for cos2 means the variable is well represented by each of the principal components.


We have chosen to maintain 3 components.This is based on our previous graphs. In the previous scree plot,based on the elbow method we see a bigger drop from the second to the third component, apart from that the variance explained by the second component is fairly similar to the one explained by the third component. As the increase in variance explained was of 10 percent, yielding up to around 70% data which is very good and this is why we decided to maintain it up until the 3rd component.
Moreover, on our most recent graph,it is easily seen that from Dim 15 to 57 there is basically no representation of the variables.In the first 3 dim, we can see different colors in most variables, this shows that they are well explained and represented. Although it is true that we could chose some more dimensions to explain better some missing variables, we want to focus on maintaining the highest amount of possible components while reducing the dimensionality as much as possible.



**Interpretation of PCA components**


```{r}
# First Component
par(mfrow = c(1,2))
barplot(pca$rotation[,1], las=2, col="darkblue")
fviz_contrib(pca, choice = "var", axes = 1)

```

This shows the most relevant variables. Which in this case are touches and carries which are really important as well as passes done, attempted and shot creating actions.These variables clearly highlights midfielders who make a lot of runs and are usually the players who most balls touch and are responsible for generating the attack in most teams.

```{r}
# Second Component
par(mfrow = c(1,2))
barplot(pca$rotation[,2], las=2, col="darkblue")
fviz_contrib(pca, choice = "var", axes = 2)
```

This second graph gives a lot of importance to goalkeeper stats and focuses on Saves, Shots Against and Clean Sheets which are goalkeeper stats for which other players have no values. It also focuses on X90s and games played, these is also related to goalkeepers as most teams have one goalkeeper who plays most matches.

```{r}
# Third Component
par(mfrow = c(1,2))
barplot(pca$rotation[,3], las=2, col="darkblue")
fviz_contrib(pca, choice = "var", axes = 3)

```

The third component is clearly for attacking positions as Expected Goals and Shots on Target are the most important variables, goals and shots also have high contributions as well as offsides which are usually committed by attackers.


Now, we will use biplots to see the different relations between the components.
As the number of variables was really high, we used different values of cos2 to filter the amount of variables to obtain a clear view of the graphs.


```{r}
# Biplot for the 1st and 2nd principal components
fviz_pca_biplot(pca, axes = c(1, 2), 
                geom.ind = "point", 
                pointsize = 3,      
                label = "var",
                select.var = list(cos2 = 0.8),
                col.var = "blue",   
                col.ind = "gray",
                repel = T,
                )  

# Biplot for the 2nd and 3rd principal components
fviz_pca_biplot(pca, axes = c(2, 3), 
                geom.ind = "point",
                pointsize = 2,
                label = "var",
                select.var = list(cos2 = 0.4),
                col.var = "purple",
                col.ind = "gray",
                repel = T)

# Biplot for the 1st and 3rd principal components
fviz_pca_biplot(pca, axes = c(1, 3), 
                geom.ind = "point",
                pointsize = 2,
                select.var = list(cos2 = 0.8),
                label = "var",
                col.var = "black",
                col.ind = "gray",
                repel = T)


```
The first graph is a really good visual example, as one one side we see many points together which are more than likely to be the goalkeepers.Then we have the rest of players joined together, by looking at the arrows it is really clear that these point in the direction of the most variance explained, the orthogonal variables are the goalkeeper variables which explain Dim2.
The second graph again has some points separated to the left, which is the same direction of the arrows. The orthogonal vectors explain mostly forwards which is why we can see a weird shape of the graph, where on the far left are the goalkeepers and on the left down part the forwards.
The third graph, is similar to the rest in that we have a bunch of points separated which are the goalkeepers, then we have the principal component 1 which clearly explain more variance with more points as it can easily be seen. This is also due to the fact that most teams have more midfielders in relation to other positions. Midfielders are usually a key part of the success of a team. The rest of points are on the direction of the orthogonal arrows, those points are the forwards.


```{r}
names <- full_data[[1]]
# The best
names[order(pca$x[,1])][1:10]
```

```{r}
names[order(pca$x[,2])][1:6]
names[order(pca$x[,2])][(length(names)-5):length(names)]
```
```{r}
names[order(get_pca_ind(pca)$contrib[,1],decreasing=T)][1:10]
```

We also want to get an insight on the best football teams, the nations with the best players as well as the team with the best players
```{r}
team <- full_data[[4]]
nation <- full_data[[2]]
comp <- full_data[[5]]

# Calculate the mean of PC1 (z1) by team
team_mean <- data.frame(z1 = pca$x[,1], team = team) %>%
  group_by(team) %>%
  summarise(mean_z1 = mean(z1)) %>%
  arrange(desc(mean_z1))

# Display the team_mean data frame
print(team_mean)

# Calculate the mean of PC1 (z1) by nation
nation_mean <- data.frame(z1 = pca$x[,1], nation = nation) %>%
  group_by(nation) %>%
  summarise(mean_z1 = mean(z1)) %>%
  arrange(desc(mean_z1))

# Display the nation_mean data frame
print(nation_mean)

# Calculate the mean of PC1 (z1) by competition
comp_mean <- data.frame(z1 = pca$x[,1], comp = comp) %>%
  group_by(comp) %>%
  summarise(mean_z1 = mean(z1)) %>%
  arrange(desc(mean_z1))

# Display the comp_mean data frame
print(comp_mean)
```
We see that the best league was the Ligue1, that the best players were from Russia and the best team was FC Barcelona




**FACTOR ANALYSIS**

Factor analysis can be very useful in this case. As we know, the factor analysis searches for latent traits in the data, and so can it do in this dataset. Hidden features could range from offensive skill and defensive capability to physical fitness. Therefore it could help us interpret the performance on players based on more broad terms rather than just individual statistics.

Combining all of this, factor analysis could be especially valuable if we decide to do any type of scouting, since we could use extracted factors to identify playmakers, dribblers or versatile players.

To finish the justification, it can also help us reduce the dimensionality of the data for training future algorithms as well as minimizing redundancy for highly correlated variables.

```{r}
library("psych")
library("GPArotation")
library("reshape2")
library("tidyr")
library("patchwork")

players.f <- fa(scaled_data, 4, rotate="promax", scores="Bartlett")
print(players.f, cut=0, digits=2)

#fa.plot(players.f)

```

```{r}
# Extract loadings and convert to a data frame
loadings <- as.data.frame(players.f$loadings[])
loadings$Variable <- rownames(loadings)

# 1st loading

loadings$Variable <- factor(loadings$Variable, levels = loadings$Variable[order(loadings$MR1, decreasing = T)])

factor1_plot <- ggplot(loadings, aes(x = Variable, y = MR1, fill = MR1 > 0)) +
  geom_col(color="black") +  # Add points for each variable's loading
  labs(title = "1st Factor Loadings for Each Variable", 
       x = "Variables", 
       y = "1st Loading") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 8),
        strip.text = element_text(size = 12),  # Increase facet title size
        plot.title = element_text(size = 16),  # Increase overall title size
        axis.title = element_text(size = 12)) +  # Increase axis title size
  scale_color_manual(values = c("red", "blue"), 
                     name = "Loading Sign", 
                     labels = c("Negative", "Positive"))


# 2nd loading

loadings$Variable <- factor(loadings$Variable, levels = loadings$Variable[order(loadings$MR2, decreasing = T)])

factor2_plot <- ggplot(loadings, aes(x = Variable, y = MR2, fill = MR2 > 0)) +
  geom_col(color="black") +  # Add points for each variable's loading
  labs(title = "2nd Factor Loadings for Each Variable", 
       x = "Variables", 
       y = "2nd Loading") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 8),
        strip.text = element_text(size = 12),  # Increase facet title size
        plot.title = element_text(size = 16),  # Increase overall title size
        axis.title = element_text(size = 12)) +  # Increase axis title size
  scale_color_manual(values = c("red", "blue"), 
                     name = "Loading Sign", 
                     labels = c("Negative", "Positive"))

# 3rd loading

loadings$Variable <- factor(loadings$Variable, levels = loadings$Variable[order(loadings$MR3, decreasing = T)])

factor3_plot <- ggplot(loadings, aes(x = Variable, y = MR3, fill = MR3 > 0)) +
  geom_col(color="black") +  # Add points for each variable's loading
  labs(title = "3rd Factor Loadings for Each Variable", 
       x = "Variables", 
       y = "3rd Loading") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 8),
        strip.text = element_text(size = 12),  # Increase facet title size
        plot.title = element_text(size = 16),  # Increase overall title size
        axis.title = element_text(size = 12)) +  # Increase axis title size
  scale_color_manual(values = c("red", "blue"), 
                     name = "Loading Sign", 
                     labels = c("Negative", "Positive"))

# 4th loading

loadings$Variable <- factor(loadings$Variable, levels = loadings$Variable[order(loadings$MR4, decreasing = T)])

factor4_plot <- ggplot(loadings, aes(x = Variable, y = MR4, fill = MR4 > 0)) +
  geom_col(color="black") +  # Add points for each variable's loading
  labs(title = "4th Factor Loadings for Each Variable", 
       x = "Variables", 
       y = "4th Loading") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 8),
        strip.text = element_text(size = 12),  # Increase facet title size
        plot.title = element_text(size = 16),  # Increase overall title size
        axis.title = element_text(size = 12)) +  # Increase axis title size
  scale_color_manual(values = c("red", "blue"), 
                     name = "Loading Sign", 
                     labels = c("Negative", "Positive"))

# Combining plots
factors_plot <- (factor1_plot + factor2_plot) / (factor3_plot + factor4_plot)
ggsave("factors_plot.png", plot = factors_plot, width = 14, height = 8, dpi = 300)
```

```{r}
fa.parallel(scaled_data)
players.f
```



**Clustering**


