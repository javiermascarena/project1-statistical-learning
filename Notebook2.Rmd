---
title: "Notebook"
output: html_document
date: "2024-11-09"
---

# BRIEF INTRODUCTION

We have chosen some datasets which represent statistics from all players in the Top 5 European Leagues which measure their performance in their domestic league in the Season 2022-2023.In this project we will analyse this datasets to obtain insights and different information about these football players.

# **SET UP**

```{r}
rm(list = ls())
# AÑADIR CODIGO PARA QUE MIRE SI EL PROFESOR TIENE EL PAQUETE INSTALADO O NO
# Importing libraries
library("dplyr")
library("skimr")
library("ggplot2")
library("ggrepel")
library("stringi")
library("GGally")
library("factoextra")
library("corrplot")
library("FactoMineR")
library("psych")
library("GPArotation")
library("reshape2")
library("tidyr")
library("patchwork")
library("ellipse")
library("RColorBrewer")
library("cluster")
library("mclust")
library("igraph")
library("pheatmap")
```

We have different datasets which contain different statistics from different players which we will need to combine to obtain a richer and more useful dataset.

```{r}
# Reading the different datasets
defense_stats <- read.csv("Big5-defenseStats22-23.csv", sep = ",")
gca_stats <- read.csv("Big5-gcaStats22-23.csv", sep = ",")
misc_stats <- read.csv("Big5-miscStats22-23.csv", sep = ",")
passing_stats <- read.csv("Big5-passingStats22-23.csv", sep = ",")
possession_stats <- read.csv("Big5-possessionStats22-23.csv", sep = ",")
shooting_stats <- read.csv("Big5-shootingStats22-23.csv", sep = ",")

# Datasets for goalkeepers
keepers_one <- read.csv("Big5-keepersStats22-23.csv", sep = ",")

# Dataset for all combined players
playingtime <- read.csv("Big5-playingtimeStats22-23.csv", sep = ",")

```

# **DATA PREPROCESSING**

We want to group all these datasets in one single one, but doing this could potentially affect our analysis as we will have a really high number of dimensions, we won't handle duplicates or NA values just yet and we will fully focus on reducing the amount of dimensions each dataset has.

Therefore, we will look at each dataset independently handling the unimportant columns or columns that can be inferred from other datasets, or columns that are linear combinations of other columns, all of these possibilites will help us reduce dimensionality. We will also rename the variables for a better understanding and clearer explanation when visualizing the data.

```{r}
# Defense Stats

#  The total amount of tackles (Tkl) is the sum of Def.3rd, Mid.3rd and Att.3rd which just refer to where in the pitch those tackles were made. Tkl.Int is a sum of both tackles and interceptions therefore we can keep this columns and eliminate both interceptions and tackles. Blocks are a sum of Sh and Pass which refers to the different type of blocks. Tkl.1 is tackles in the first half, already included in the first variable. Tkl.1 measures the percentage of lost balls when Att is done, it divides Att and Lost. Many players dont have errors, and at the same time errors leading to goal are mostly done by defenders as an error in the defense side is more crucial, therefore we will eliminate it as well.

defense_cols <- c("Def.3rd","Mid.3rd","Att.3rd","Tkl.1","Int","Sh", "Pass", "Att", "Lost","Err")

defense_stats <- defense_stats %>% select(-all_of(defense_cols))

defense_stats <- defense_stats %>% 
  rename(TacklesDone = Tkl, SuccessfulTackles = TklW, TacklesANDIntercep = Tkl.Int,
         Clearances = Clr, PercDribblersTackled = Tkl.)


# GCA stats

# SCA is an advanced metric that tracks the two offensive/attacking actions that directly lead to a shot on # goal, Following the same logic as a Shot-Creating Action, a Goal-Creating Action is an advanced metric that tracks the two offensive/attacking actions that leads to a goal. These follow the same criteria as  the above list. The difference is that one causes a goald, the other an scoring chance

# SCA is a sum of PassLive, PassDead, TO, Sh Fld, Def, they are different stats that contribute to it, and  SCA90s can be inferred as SCA*90s returns SCA90s
# GCA follows the same deduction as before

gca_cols <- c("SCA90","PassLive", "PassDead", "TO", "Sh", "Fld", "Def", "GCA90", "PassLive.1", "PassDead.1", "TO.1", "Sh.1", "Fld.1", "Def.1")

gca_stats <- gca_stats %>% select(-all_of(gca_cols))

gca_stats <- gca_stats %>% 
  rename(ShotCreatingActions = SCA, GoalCreatingActions = GCA )

# Misc stats

# X2CrdY is when the player is suspended by 2 yellow cards instead of one red card, this is included in the Red Card statistic. TklW and Int are already included in the defense dataset, Won. refers to the aerial duels won, and it divides won and lost, so we can remove them. Own goals are many times fortunate and dont affect the skill value of a player. PKwon and PKcon refer to the amount of penalties conceded and won, but not many players have a value here so we will remove it as it is irrelevant. Crs refers to crosses and will appear in a future dataset 

misc_cols <- c("X2CrdY", "TklW", "Int", "Won", "Lost", "OG", "PKwon", "PKcon", "Crs")


misc_stats <- misc_stats %>% select(-all_of(misc_cols))

misc_stats <- misc_stats %>% 
  rename(YellowCards = CrdY, RedCards = CrdR, FoulsCommited = Fls, FoulsDrawn = Fld, Offsides = Off, RecoveredBalls = Recov, PercAerialDuelsWon = Won.)

# Passing Stats

#Cmp. is the percentage of the completed passes in terms of the attempted ones, therefore we can remove them. Cmp.1, Att.1, Cmp..1,Cmp.2,Att.2, Cmp..2, Cmp.3,Att.3,Cmp..3 are all relative to the distance of the passes, which is not of much interest as of right now and both TotDist and PrgDist contain a lot of information about the distance of passes. 1/3, PPA, CrsPa, PrgP are also covered as they all refer to progressive passes. A.xAG is assists minus the xAG, therefore we can remove it.

passing_cols <- c("Cmp.","Cmp.1", "Att.1", "Cmp..1","Cmp.2","Att.2", "Cmp..2", "Cmp.3","Att.3","Cmp..3","X1.3", "PPA", "CrsPA", "PrgP","A.xAG")

passing_stats <- passing_stats %>% select(-all_of(passing_cols))

passing_stats <- passing_stats %>% 
  rename(CompletedPasses = Cmp, AttemptedPasses = Att, PassingDistance = TotDist, ForwardPassingDistance = PrgDist, Assists = Ast, ExpectedAssistsGoal = xAG, ExpectedAssists = xA,  KeyPasses = KP)

# Posession Stats


# Touches is a sum of Def.Pen, Def.3rd, Mid.3rd, Att.3rd, Att.Pen, Live so we can remove them. Succ. is the division of Att and Succ which refers to the number of times the attacker has dribbled the defender divided by the total attemps. Tkld, Tkld. are two variables directly related to the previous so we can also take them out. X1.3, CPA, Mis are not really valuable. TotDist and PrgDist will have different names to differentiate from the previous dataset that used the same variables. Rec, PrgR refer to the times the player received a good pass where in most of the cases if a pass is missed it is usually the passers fault rather than the receiver.


posession_cols <- c("Def.Pen", "Def.3rd", "Mid.3rd", "Att.3rd", "Att.Pen", "Live", "Att", "Succ", "Tkld", "Tkld.","X1.3", "CPA", "Mis", "Rec", "PrgR", "PrgC")

possession_stats <- possession_stats %>% select(-all_of(posession_cols))

possession_stats <- possession_stats %>% 
  rename(DistanceRanBall = TotDist, ForwardDistanceRanBall = PrgDist, PercSuccessfulDribbles = Succ., Dispossessed = Dis)

# Shooting Stats

# With goals, shots and shots on target we can infer SoT., Sh.90, SoT.90, G.Sh, G.SoT. As not many players shoot penalties or free kicks we will not take into account this, FK, PK and PKatt will be removed. npxG/Sh can be calculated with both variables, as well as G.xG and np.G.xG can also be calculated with npxG and xG

shooting_cols <- c("SoT.", "Sh.90", "SoT.90", "G.Sh", "G.SoT","FK", "PK","PKatt", "npxG.Sh", "G.xG", "np.G.xG")

shooting_stats <- shooting_stats %>% select(-all_of(shooting_cols))

shooting_stats <- shooting_stats %>% 
  rename(Goals = Gls, Shots = Sh, ShotsOnTarget = SoT, ShotDistance = Dist, ExpectedGoals = xG, NonPenaltyExpectedGoals = npxG)

# Keepers one

# The statistics MP, Starts, Min will be in another dataset so we will remove them. Save. is calculated by SoTA-GA/SoTa and CS. which refers to clean sheets can also be removed. Everything about free kicks and penalties can be removed as it is easy to score from a penalty and harder from a free kick and it usually depends on the quality of the attacker, many times the keeper cannot do much.We will remove PKatt, PKA, PKsv,PKm,Save..1. GA90 can also be inffered with Goals Against and 90s

keepers_cols <- c("MP", "Starts", "Min", "Save.", "CS.", "PKatt", "PKA", "PKsv","PKm","Save..1", "GA90")

keepers_one <- keepers_one %>% select(-all_of(keepers_cols))

keepers_one <- keepers_one %>% 
  rename(GoalsAllowed = GA, ShotsOnTargetAgainst = SoTA, Won = W, Drawn = D, Lost = L, CleanSheets = CS)



# Playing Time

# Min can be calcualted by 90 * X90S.Mn.Mp can be calculated as Min/Mp, Mn. can also be calculated. Mn.Start will be Min/Starts, Compl can be seen as it is not really important. The same for all sub variables as with minutes we can already kind of see the amount of time played. We will remove Subs, Mn.Sub, unSub. X... is substraction of onG and onGA so we can remove it, and the same for the 90s, X...90. Same for On.Off.1 and On.Off. Minutes is a character therefore we will have to change to numeric

playing_cols <- c("Mn.MP","Min.","Min", "Mn.Start", "Compl", "Subs", "Mn.Sub", "unSub","X...", "X...90", "On.Off", "On.Off.1", "xG...", "xG...90")

playingtime <- playingtime %>% select(-all_of(playing_cols))

playingtime <- playingtime %>% 
  rename(MatchesPlayed = MP, PointsPerMatch = PPM, GoalsByTeam = onG, GoalsAgainstTeam = onGA, ExepectedGoalsByTeam = onxG, ExpectedGoalsAgainstTeam = onxGA )
```

Merging all datasets according to different columns and dealing with missing values and duplicates later, this can be done with the inner join function taken into account that all datasets have the same 8 first columns.

```{r}
# Making a list with the repeated columns
repeated_columns = c("Player", "Nation", "Pos", "Squad", "Comp", "Age", "Born", "X90s")


# Inner joining all datasets based on key columns and a brief look at it
merged_players <- inner_join(defense_stats, gca_stats, by = repeated_columns)%>%
                  inner_join(misc_stats, by = repeated_columns)%>%
                  inner_join(passing_stats, by = repeated_columns)%>%
                  inner_join(possession_stats, by = repeated_columns)%>%
                  inner_join(shooting_stats, by = repeated_columns)

skim(merged_players)


```

We now have the problem that goalkeepers have different stats to players, therefore the number of variables varies and we cannot join them to the same dataset. Instead we need to add the columns which are not in players to goalkeepers with a 0 value for each entry and viceversa.

```{r}
# Obtaining all columns in players and goalkeepers
player_columns <- colnames(merged_players)
gk_columns <- colnames(keepers_one)

# Finding out the different columns in each of the datasets
missing_columns_players = setdiff(gk_columns, player_columns)
missing_columns_gk = setdiff(player_columns, gk_columns)

# Using a for loop to create the missing columns with a value of 0
for (col in missing_columns_players) {
  merged_players[[col]] <- 0
}

for (col in missing_columns_gk) {
  keepers_one[[col]] <- 0
}

# Combining both goalkeepers and players
all_players = bind_rows(merged_players,keepers_one)

# Merging with playing time stats
final_players <- inner_join(all_players, playingtime, by = repeated_columns)
head(final_players)

```

We observe many redundancies in this new dataset, such as that the nations are repeated in both lower and upper case, that some players have more than one position or that the competition also has lower case in front which could be troubling in the future. We will remove the strings with gsub and substr, and maintain only the first position for all players.

```{r}
# Removing lower case in nation
final_players$Nation <- gsub("[a-z]", "", final_players$Nation)
# Saving only the first position
final_players$Pos <- substr(final_players$Pos,1,2)
# Removing lower case before leagues
final_players$Comp <- substr(final_players$Comp, 4, nchar(final_players$Comp))


# With that solved we will look at the players now to check for NAs and duplicated values
head(final_players)
```

Apart from that we also see there are a total of +200 duplicates which could be from players who were transferred or were loaned mid-season. To deal with this, we will keep the entries with the most time played.

```{r}
# Removing duplicate players
final_players <- final_players %>%
  group_by(Player) %>%          
  # Keep the observation with the most time played
  slice_max(order_by = X90s, n = 1) %>%    
  ungroup()

# Check that there are no duplicates now
sum(duplicated(final_players))
```

**NA Values**

Using a barplot to see the columns with NA values.

```{r}
barplot(colMeans(is.na(final_players)), las=2)
```

**Median Imputation**

Since all NA values are in numerical columns, we can replace these NAs by the median of that column.

```{r}
# Replace NA values with the median of each column
final_players <- final_players %>%
  mutate(across(everything(), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))

# Number of rows with NA values 
nrow(final_players %>% filter(if_any(everything(), is.na)))
```

We see that this dataset has many statistics for players, but we could also benefit from other ones that are not there, such as the overall player rating or their price. For this we will merge part of another dataset which bases its rating and some other statistics in the FIFA videogame.

```{r}
# Reading complimentary data and removing unnecessary columns
data_fifa <- read.csv("CLEAN_FIFA23_official_data.csv")
head(data_fifa)
colnames(data_fifa)

# Checking for duplicate values
sum(duplicated(data_fifa$Name))
```

As it has happened with our previous football datasets, we will have to modify duplicate values

```{r}
# Obtaining the data without repeated players
data_fifa <- data_fifa %>%
  group_by(Name) %>%
  # Keeping only observations with the most recent joined date
  slice_max(Joined, n = 1, with_ties = FALSE) %>%  
  ungroup()

# Check that there are no duplicates now
sum(duplicated(final_players))


```

We can also see that the dataset is already clean with no NA values but looking at the different columns and variables there are some columns which do not provide much information, are already in the previous dataset or are not of our interest for our current project, therefore, we will remove them. Since the merging will be done by name it is the only column present in other datasets that we will maintain. The names of the players in the fifa dataset has numbers before which can lead to errors when merging as well, so we will have to remove it.

```{r}
colnames(data_fifa)

cols <- c("X", "ID", "Age", "Photo","Flag", "Club", "Club.Logo", "Special","Work.Rate","Skill.Moves", "Weak.Foot","Body.Type", "Real.Face", "Position", "Joined", "Loaned.From", "Contract.Valid.Until","Release.Clause...", "Kit.Number", "Best.Overall.Rating", "Year_Joined", "Potential", "Preferred.Foot", "International.Reputation", "Nationality")

data_fifa <- data_fifa %>% select(-all_of(cols))

data_fifa$Name <- gsub("[0-9]+", "", data_fifa$Name)

```

Now that we have cleaned a bit the other dataset we can proceed to merging it with this new and last one.

For this there is a problem, where FIFA players have different name formats ("Messi", "L. Messi" or "Lionel Messi"). To solve this we will first standardized name to the format "L. Messi" and then extract the surnames for both datasets and join by means of the surname.

Also we will replace before all of this all characters from other languages into the closest one in the English alphabet ("è" -\> "e")

```{r}
# Replacing characters by its closest English alphabet character
final_players$Player <- stri_trans_general(final_players$Player, "latin-ascii")
data_fifa$Name <- stri_trans_general(data_fifa$Name, "latin-ascii")

# Handle additional characters manually for specific languages 
final_players$Player <- gsub("ć|č", "c", final_players$Player)
final_players$Player <- gsub("ł", "l", final_players$Player)
final_players$Player <- gsub("ń", "n", final_players$Player)
final_players$Player <- gsub("ă", "a", final_players$Player)
final_players$Player <- gsub("ș|ş", "s", final_players$Player)
final_players$Player <- gsub("ž|ź", "z", final_players$Player)

# Removes other symbols
final_players$Player <- gsub("[^[:alnum:] ]", "", final_players$Player)  

data_fifa$Name <- gsub("ć|č", "c", data_fifa$Name)
data_fifa$Name <- gsub("ł", "l", data_fifa$Name)
data_fifa$Name <- gsub("ń", "n", data_fifa$Name)
data_fifa$Name <- gsub("ă", "a", data_fifa$Name)
data_fifa$Name <- gsub("ș|ş", "s", data_fifa$Name)
data_fifa$Name <- gsub("ž|ź", "z", data_fifa$Name)
data_fifa$Name <- gsub("[^[:alnum:] ]", "", data_fifa$Name)



# Set a Name column to the format "L. Messi"
data_fifa$StandardizedName <- paste0(substr(data_fifa$Name, 1, 1), ". ", sapply(strsplit(data_fifa$Name, " "), function(x) x[length(x)]))

# Create a new column of the surname
final_players$Surname <- sapply(strsplit(final_players$Player, " "), function(x) x[length(x)])
data_fifa$Surname <- sapply(strsplit(data_fifa$StandardizedName, " "), function(x) x[length(x)])

# Merge both datasets by Surname
full_data <- inner_join(final_players, data_fifa, by = "Surname")

# Eliminating repeated values when merging has not been correct
full_data <- na.omit(full_data)
full_data <- full_data %>%
  group_by(Player) %>%          
  # Keep the observation with the most time played
  slice_max(order_by = GoalsAllowed, n = 1, with_ties = FALSE) %>%    
  ungroup()

names = full_data[,1]
```

We observe that some observations have been lost. If we look closely at the data most of those observations not merged do not appear in the FIFA dataset so, since they are not very relevant we can exclude them confidently.

```{r}
# Observations not merged
not_merged <- anti_join(final_players, full_data, by = "Player")
```

**Outliers**

Outliers are actually important as many players could have a fantastic season compared to others so it is important to keep them. But there are many players who are not in the managers rotation and they do not play much minutes so we will eliminate them as they will not provide useful information.

```{r}
ggplot(full_data) +
  aes(x = X90s)+ 
  geom_area(stat = "bin",fill="lightblue", color="blue", binwidth = 40) + 
   theme_minimal()
```

X90s is a representation of the total minutes played divided by 90, which shows more or less the amount of games played. Most leagues have a total of 38 games of 90 minutes, with the added minutes, it makes sense the maximum value is 40. But what is really useful here is that there are many players who have not even played a single game and therefore will be irrelevant in our project and analysis, so we will remove them.

```{r}

# Eliminating players that have not played much
full_data <- full_data[full_data$X90s > 2,]
```

**Exploratory Data Analysis**

We will now create some graphs to generate some insights and obtain information about the players.

Focusing now on goals and assists, as they are one of the most important variables as goals and assists make teams win games, we create this graph.

In this plot, we can see as expected that forwards have higher values, it is also remarkable that we have some goalkeepers with an assist or goal. Players above the line have scored less goals than expected while players further to the right and below the line have scored more goals than it was expected. Therefore it is desirable to be on the bottom right of the plot for great goals and assists conversion.

Also we can clearly see that there is a linear relationship between both variables.

```{r}
plot1 <- ggplot(full_data)+
  aes(x = Assists+Goals, y = ExpectedGoals+ExpectedAssists, color = Pos)+
  geom_point(alpha=0.85)+
  geom_text_repel(
    data = subset(full_data, Assists + Goals > 25), 
    aes(label = Player),
    size = 2
  ) +
  geom_abline()+
  ggtitle(" Goals and Assists vs Expected Goals and Assists")+
  xlab("Total Goals + Assists") +                      
  ylab("Expected Goals + Assists") + 
  theme(plot.title = element_text(hjust = 0.5))

# Save the plot 
#ggsave("goals_assists_plot.png", plot = plot, width = 10, height = 8, dpi = 300)

```

In this plot we focus on offensive players. Usually players that are more game-changing are targeted by opposing players, and fouling them is one of the most common strategies to stop them. This is why in this plot we compare fouls drawn and the success chance in dribbles.

In the plot we can see that players that are good dribblers are usually the most fouled, and also have many carries. The biggest example of this is Vinicius Jr.

```{r}
plot2 <- ggplot(full_data)+
  aes(x = PercSuccessfulDribbles, y = FoulsDrawn, color = Pos, size=Carries)+
  geom_point(alpha = 0.85)+
  scale_size_continuous(range = c(1, 8)) +
  geom_text_repel(
    data = subset(full_data, FoulsDrawn > 70), 
    aes(label = Player),
    size = 2
  ) +
  ggtitle("Successful Dribble % vs Fouls Drawn")+
  xlab("Successful Dribble %") +                      
  ylab("Fouls Drawn") + 
  theme(plot.title = element_text(hjust = 0.5))

# Save the plot 
#ggsave("dribbles_fouls_plot.png", plot = plot, width = 10, height = 8, dpi = 300)
```

In this plot we focus on passes. With this density graph, we observe the range where most players fall into, but also we observe that as players tend to pass more, their accuracy also increases.

```{r}
plot3 <- ggplot(full_data, aes(x=100*CompletedPasses/AttemptedPasses, y=PassingDistance/X90s)) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon", colour="white")+
  ggtitle("Pass Accuracy % vs Passing Distance per Game")+
  xlab("Pass Accuracy %") +                      
  ylab(" Avg. Passing Distance per Game") + 
  theme(plot.title = element_text(hjust = 0.5))

#ggsave("pass_perc_distance_plot.png", plot = plot, width = 10, height = 8, dpi = 300)
```

Next, we created a correlation plot for defensive actions. As we can see except Aerial Duels, the other ones are closely related. However, this defensive actions are more correlated in midfielders, and sometimes forwards, than in defenders which is surprising.

```{r}
# Select only the relevant columns for the correlogram
correlogram_data <- full_data[, c("SuccessfulTackles", "Blocks", "FoulsCommited", "RecoveredBalls", "PercAerialDuelsWon")]

# Create the correlogram
plot4 <- ggpairs(correlogram_data, 
        title = "Correlogram of Defensive Actions",
        lower = list(continuous = wrap("points", alpha = 0.3)),
        upper = list(continuous = wrap("cor", size = 4)), 
        ggplot2::aes(colour=full_data$Pos)) +
  theme(plot.title = element_text(hjust = 0.5))  # Center the title

#ggsave("defense_corr_plot.png", plot = plot, width = 10, height = 8, dpi = 300)
```

Here we study goalkeepers. As we can see as the save % increases, the clean sheets also increase but the relation is not that clear, also there are some clear outliers, such as MarcAndre ter Stegen.

```{r}
plot5 <- ggplot(full_data)+
  aes(x = 100*Saves/ShotsOnTargetAgainst, y = CleanSheets, color = Comp)+
  geom_point(alpha = 0.85)+
  geom_text_repel(
    data = subset(full_data, Saves/ShotsOnTargetAgainst > 0.6 & CleanSheets >= 10), 
    aes(label = Player),
    size = 2
  ) +
  ggtitle("Save % vs Clean Sheets")+
  xlab("Save %") +                      
  ylab("Clean Sheets") + 
  theme(plot.title = element_text(hjust = 0.5))

# Save the plot 
#ggsave("goalkeepers_plot.png", plot = plot, width = 10, height = 8, dpi = 300)
```

# **Descriptive Analysis**

Before using any algorithm on the data we have to check whether or not we scale the data, and also select the numeric columns and the ones we are interested in. We will start by choosing some numeric columns and plotting them to see their distribution.

```{r}

numeric_columns <- full_data[, c("X90s", "TacklesDone", "SuccessfulTackles", "PercDribblersTackled", "Blocks", "TacklesANDIntercep", "Clearances", "ShotCreatingActions", "GoalCreatingActions","PointsPerMatch","GoalsByTeam", "GoalsAgainstTeam")] 

par(mfrow = c(3, 3)) # Adjust the layout for 4x4 grid to fit all histograms

for (col_name in names(numeric_columns)) {
  hist(
    numeric_columns[[col_name]], 
    main = paste("Histogram of", col_name), 
    xlab = col_name, 
    col = "skyblue", 
    border = "white"
  )
}

```

Most of these plots are left skewed, one reason for this is due to the number of goalkeepers, as these position has no value for these variables recorded. For goalkeeper stats it would happen exactly the same. Apart from that, most teams may have around 25 players, while the lineups are of 11 players, these leads to difficulties distributing the number of minutes between the players so the recorded stat for most players is low. Now we will use all numeric variables and see the relations and information we can obtain.

```{r}
# Was used to show the different histograms, now we have to delete it
par(mfrow = c(1, 1))


# Need to select numeric data from the dataset
numeric_data <- select_if(full_data, is.numeric)

# Using boxplots to visualize a few columns
boxplot(numeric_columns, las=2, col="darkblue")

```

These boxplots clearly show that we will need to scale the data before performing any algorithm on them. This is due to the difference of variance between each variables.

As the data is mostly left skewed, to fix this we can apply a root to the numeric columns.

```{r}

# Scaling the data
scaled_data <- scale(sqrt(numeric_data)) #Numeric columns no numeric data

boxplot(scaled_data, las=2, col="darkblue")

```

The variables returned are now scaled, so there will be no big differences in variances. There are some very small boxplots in the middle, this is because this statistics refer to goalkeepers, where all players have a 0, and therefore the amount of outliers is big, but it actually corresponds to goalkeepers.

Let's look at the correlation between the different variables, to see if we can obtain some insights before running any algorithm. Using the corrplot library for a better understanding.

```{r}
correlation_matrix = cor(numeric_data)

corrplot(correlation_matrix, 
         method = "ellipse",       
         type = "lower",           
         order = "hclust",         
         tl.col = "black",         
         tl.srt = 45,              
         tl.cex = 0.5,             
         cl.cex = 0.5)

```

This shows the correlation for the different variables, some of these have high correlation which could affect the PCA. There are also some variables with no or very low correlation which is why the PCA could offer more information about the variables.

# **PRINCIPAL COMPONENT ANALYSIS**

The main objective of PCA is to visualize a high dimensional space in lower dimensions, where the new reduced variables (principal components) will be uncorrelated and linear combinations of the original one. This will also reduce redundancy and find relationships between variables.

The first principal component will show the direction of the variance. This component will be given by these formula:

$$  Z_1 = X \mathbf{a}_1 $$

Where here $X$ represents the data matrix, $Z_1$ the first components and $\mathbf{a}_1$ a vector of loadings that shows the direction of the original feature map.

As we want to obtain the maximum information possible, we will maximize the variance of this component. The variance is given by these formula:

$$
s_{z_1}^2 = \mathbf{a}_1^T S \mathbf{a}_1
$$

Where $S$ represents the covariance matrix of the data and $\mathbf{a}_1$ a unit vector so it is not affected by the scaling of variables. Scaling is important in PCA as we want to center every variable, because if not elements with a higher scale will have a larger variance affecting our analysis.

As $S$ is a covariance matrix we can decompose it by:

$$
S \mathbf{v} = \lambda \mathbf{v}
$$

With this formula and the previous taking into account that $\mathbf{a}_1$ is a unit vector, we will obtain that the maximum variance for the first component will be its eigenvalue. The direction will be given by $\mathbf{a}_1$ , its eigenvector. This exact procedure will be the same for the rest of components. As two uncorrelated variables have covariance 0, the next component will be orthogonal. The matrix with the principal components score will be given by:

$$
Z = X A_r
$$

Where $Z$ is the matrix of the principal components, $X$ the data matrix and $A_r$ the principal component loadings.

PCA will be essential in our project as our current dataset has around 60 dimensions, so reducing these number of dimensions to have a more visual and clearer representation of data could be crucial.

```{r}
#Scale is set to False as the data is already scaled and this could lead to errors
pca = prcomp(scaled_data, scale=F)

summary(pca)
get_eigenvalue(pca)

```

## **Choosing the number of components**

To select the number of clusters we will build up from the Kaiser Criterion. This criterion states that only the principal components with eigenvalues higher than 1 should appear in the analysis. It is of value greater than 1 as since each eigenvalue represents the variance contributed, a value lower than 1 would mean it explains less than the variables, so it is not useful.

However, this method does not deal well with noise, therefore we will use parallel analysis which will solve this issue. What parallel analysis does is that it compares the eigenvalues of the actual data with the ones generated with random data following the structure of the original data.

```{r}
# Using factoextra package to see the variances explained by each component
fviz_eig(pca, addlabels = TRUE)

# Parallel analysis plot
fa.parallel(scaled_data, fa = "pc", show.legend = TRUE, main = "Parallel Analysis for PCA", sqrt=F)

# This line here returns 57 eigenvalues, one per column, it explains the amount of variance explained by each component. It also returns the eigenvectors, which show the value of each variable in each component.
get_eigenvalue(pca)

```

2 dimensions explain 59.7% of the variability while 3 dimensions explain 71.11%.

This next graph uses cos\^2, which refers to the quality of representation of the different variables by each principal component. It returns how much of the variable\`s variance is captured by that component.It returns values between 0 and 1 so it will be easy to interpret. A high value for cos2 means the variable is well represented by each of the principal components.

```{r}
var <- get_pca_var(pca)
corrplot(
  var$cos2, 
  is.corr = FALSE, 
  method = "color",               
  type = "lower",                 
  tl.cex = 0.5,                   
  tl.srt = 45,                    
  cl.cex = 0.5,
  tl.col = "black",
  title = "Representation of Variables by Each Component",
  mar = c(0, 0, 2, 0))

```

We have chosen to maintain 7 components.This is based on our previous graphs. In the previous scree plot,apart from the eigenvalues obtained we see that the variance explained by 7 components is 85.34% which is a really high value. Apart from that, parallel analysis returned a value of 7 confirming and giving more backup to our statement. Moreover, on our most recent graph,it is easily seen that from Dim 15 to 57 there is basically no representation of the variables. Most variables are well represented in the first few dimensions , and all variables are represented in some way when the 7th dimension is reached.As we started with dimension 57 and we want to reduce dimensionality as much as possible while retaining as much variance as possible, the election of 7 components seems to be pretty good.

# **Interpretation of PCA components**

```{r}
# First Component
par(mfrow = c(1,2))
barplot(pca$rotation[, 1], 
        horiz = TRUE,        
        col = "darkblue",
        main = "First Principal Component",
        xlab = "Loading",
        las = 1,             
        cex.names = 0.5)
fviz_contrib(pca, choice = "var", axes = 1)

# Second Component
par(mfrow = c(1,2))
barplot(pca$rotation[, 2], 
        horiz = TRUE,        
        col = "darkblue",
        main = "Second Principal Component",
        xlab = "Loading",
        las = 1,             
        cex.names = 0.5)
fviz_contrib(pca, choice = "var", axes = 2)

# Third Component
par(mfrow = c(1,2))
barplot(pca$rotation[, 3], 
        horiz = TRUE,        
        col = "darkblue",
        main = "Third Principal Component",
        xlab = "Loading",
        las = 1,             
        cex.names = 0.5)
fviz_contrib(pca, choice = "var", axes = 3)

# Fourth Component
par(mfrow = c(1,2))
barplot(pca$rotation[, 1], 
        horiz = TRUE,        
        col = "darkblue",
        main = "Fourth Principal Component",
        xlab = "Loading",
        las = 1,             
        cex.names = 0.5)
fviz_contrib(pca, choice = "var", axes = 4)

# Fifth Component
par(mfrow = c(1,2))
barplot(pca$rotation[, 1], 
        horiz = TRUE,        
        col = "darkblue",
        main = "Fifth Principal Component",
        xlab = "Loading",
        las = 1,             
        cex.names = 0.5)
fviz_contrib(pca, choice = "var", axes = 5)

# Sixth Component
par(mfrow = c(1,2))
barplot(pca$rotation[, 6], 
        horiz = TRUE,        
        col = "darkblue",
        main = "Sixth Principal Component",
        xlab = "Loading",
        las = 1,             
        cex.names = 0.5)
fviz_contrib(pca, choice = "var", axes = 6)

# Seventh Component
par(mfrow = c(1,2))
barplot(pca$rotation[, 7], 
        horiz = TRUE,        
        col = "darkblue",
        main = "Seventh Principal Component",
        xlab = "Loading",
        las = 1,             
        cex.names = 0.5)
fviz_contrib(pca, choice = "var", axes = 7)

```

This shows the most relevant variables.

For the first component, these are touches and carries which are really important as well as passes done, attempted and shot creating actions.These variables clearly highlights midfielders who make a lot of runs and are usually the players who most balls touch and are responsible for generating the attack in most teams.

The second component gives a lot of importance to goalkeeper stats and focuses on Saves, Shots Against and Clean Sheets which are goalkeeper stats for which other players have no values. It also focuses on X90s and games played, these is also related to goalkeepers as most teams have one goalkeeper who plays most matches.

The third component is clearly for attacking positions as Expected Goals and Shots on Target are the most important variables, goals and shots also have high contributions as well as offsides which are usually committed by attackers

The fourth component are clearly the most famous players as they have the highest overall, as well as the highest value and wage, which are clearly the most relevant variables here.

The fifth component correspond to the weight and height, so this component will focus more on the physical conditions of the players.

The sixth component gives a lot of value to the age, both the oldest and youngest players.

Finally, the seventh component has a lot of value for points per match as well as overall, so usually the best players who provide a lot for the team, especially points

Now, we will use biplots to see the different relations between the components. As the number of variables was really high, we used different values of cos2 to filter the amount of variables to obtain a clear view of the graphs. Due to the high amount of components selected (7), and the fact that many of these components explained low variance in comparison to the first 3, we created a few biplots of the first 4 components with the first one which we think will be more than enough to see the relationship of data and obtain a good insight.

```{r}
# Biplot for the 1st and 2nd principal components
fviz_pca_biplot(pca, axes = c(1, 2), 
                geom.ind = "point", 
                pointsize = 3,      
                label = "var",
                select.var = list(cos2 = 0.8),
                col.var = "blue",   
                col.ind = "gray",
                repel = T,
                )  

# Biplot for the 2nd and 3rd principal components
fviz_pca_biplot(pca, axes = c(1, 3), 
                geom.ind = "point",
                pointsize = 2,
                label = "var",
                select.var = list(cos2 = 0.85),
                col.var = "purple",
                col.ind = "gray",
                repel = T)

# Biplot for the 1st and 3rd principal components
fviz_pca_biplot(pca, axes = c(1, 4), 
                geom.ind = "point",
                pointsize = 2,
                select.var = list(cos2 = 0.7),
                label = "var",
                col.var = "black",
                col.ind = "gray",
                repel = T)


```

The first graph is a really good visual example, as one one side we see many points together which are more than likely to be the goalkeepers.Then we have the rest of players joined together, by looking at the arrows it is really clear that these point in the direction of the most variance explained, the orthogonal variables are the goalkeeper variables which explain Dim2.

The second graph, is similar due to that we have a bunch of points separated which are the goalkeepers, then we have the principal component 1 which clearly explain more variance with more points as it can easily be seen. This is also due to the fact that most teams have more midfielders in relation to other positions. Midfielders are usually a key part of the success of a team. The rest of points are on the direction of the orthogonal arrows, those points are likely to be the forwards.

The third grah, following the same pattern as before has some separate points which are more than likely to be the goalkeepers.Apart from that most of the variance is explained by the first component which are midfielders and therefore there are more points as most variance is being explained, while the fourth component took into account the wage, so points which are below are higher paid footballers.

### Interpretation of PCA for individuals

We will now focus on individuals, leagues and nations to see which players most strongly represent the variance on the first components. Basically, the players with the best statistics of the variables that are better explained by the first principal components.

```{r}
names <- full_data[[1]]

print("First Component")
names[order(-pca$x[,1])][1:10]
# The sign here for the pca is negative, as the goalkeeper stats are inversely proportional and if not we would be getting goalkeepers instead

print("Second Component")
names[order(pca$x[,2])][1:10]

print("Third Component")
names[order(pca$x[,3])][1:10]
```

These individuals actually show our analysis of the PCA was correct as the players obtained in each of the components have the same position and characteristics we were mentioning in our interpretation above.

We also want to get an insight on the best football teams, the nations with the best players as well as the team with the best players

```{r}
team <- full_data[[4]]
nation <- full_data[[2]]
comp <- full_data[[5]]

# Calculate the mean of PC1 (z1) by team
team_mean <- data.frame(z1 = pca$x[,1], team = team) %>%
  group_by(team) %>%
  summarise(mean_z1 = mean(z1)) %>%
  arrange(desc(mean_z1))

# Display the team_mean data frame
print(team_mean)

# Calculate the mean of PC1 (z1) by nation
nation_mean <- data.frame(z1 = pca$x[,1], nation = nation) %>%
  group_by(nation) %>%
  summarise(mean_z1 = mean(z1)) %>%
  arrange(desc(mean_z1))

# Display the nation_mean data frame
print(nation_mean)

# Calculate the mean of PC1 (z1) by competition
comp_mean <- data.frame(z1 = pca$x[,1], comp = comp) %>%
  group_by(comp) %>%
  summarise(mean_z1 = mean(z1)) %>%
  arrange(desc(mean_z1))

# Display the comp_mean data frame
print(comp_mean)
```

We see that the league with the players who had statistics that best represented the variance was the Ligue1.Nation wise it was Russia and club wise it was FC Barcelona

# **FACTOR ANALYSIS**

# **Motivation**

Factor analysis can be very useful in this case. As we know, the factor analysis searches for latent traits in the data, and so can it do in this dataset. Hidden features could range from offensive skill and defensive capability to physical fitness. Therefore it could help us interpret the performance on players based on broader terms rather than just individual statistics.

Combining all of this, factor analysis could be especially valuable if we decide to do any type of scouting, since we could use extracted factors to identify playmakers, dribblers or versatile players.

To finish the justification, it can also help us reduce the dimensionality of the data for training future algorithms as well as minimizing redundancy for highly correlated variables.

# **Number of factors**

Before performing factor analysis, one of the key metrics to decide, is the number of factors we want to extract from the data. For this we will use parallel analysis, a method on which new random data is generated from our original data, and the eigenvalues of the actual and the new random data are compared.

As we know in factor analysis we have the covariance matrix:$$ \Sigma = LL^T + \Psi $$ In reality as factors might be correlated, this matrix can be decomposed as:$$ \Sigma = L Cov(F) L^T + \Psi$$where $L$ is the factor loading matrix , $Cov(F)$ the covariance matrix of factors (with eigenvalues on the diagonal) and $\Psi$ represents the covariance matrix of the unobserved stochastic error.

To assess whether or not the factor is significant, what we do is compare the eigenvalues (variance explained by each factor) with the mean eigenvalue from the random data. Then if at the corresponding position $\lambda < \mathbb{E}[\lambda_{\text{random}}]$ we can say that this factor does not explain more variability than what would be expected of random noise, so we will not use this factor.

```{r}
# Parallel analysis plot
fa.parallel(scaled_data, fa = "fa", show.legend = TRUE, main = "Parallel Analysis for Factor Analysis", sqrt=T)
```

As we observe the real eigenvalues stay above the ones of random data, for factors from 1 to 8, so we will use 8 factors for factor analysis.

Before doing the factor analysis there are two choices have to be made. For rotation we chose "oblimin" since we know from before that many variables are really correlated, therefore the rotation matrix does not force the factors to be orthogonal. For score we chose "regression" since it gives the most accurate and unbiased estimations of factors when they may be correlated.

# **Training**

```{r}
# Performing factor analysis and extracting 8 factors
players.f <- fa(scaled_data, 8, rotate="oblimin", scores="regression")
print(players.f, cut=0, digits=2)
```

# **Factor explanation**

As we can see from the output the total variance explained by all factors is 85%, but we can also observe that the variance explained by each factor alone decreases until it goes flat in about a 5% for the last three.

To comprehend better how each factor is impacted by all variables, we make the following plot.

```{r}
# Extract loadings and convert to a data frame
loadings <- as.data.frame(players.f$loadings[])
loadings$Variable <- rownames(loadings)

# 1st loading

loadings$Variable <- factor(loadings$Variable, levels = loadings$Variable[order(loadings$MR1, decreasing = T)])

factor1_plot <- ggplot(loadings, aes(x = Variable, y = MR1, fill = MR1 > 0)) +
  geom_col(color="black") +  # Add points for each variable's loading
  labs(title = "1st Factor Loadings for Each Variable", 
       x = "Variables", 
       y = "1st Loading") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 8),
        strip.text = element_text(size = 12),  # Increase facet title size
        plot.title = element_text(size = 16),  # Increase overall title size
        axis.title = element_text(size = 12)) +  # Increase axis title size
  scale_color_manual(values = c("red", "blue"), 
                     name = "Loading Sign", 
                     labels = c("Negative", "Positive"))


# 2nd loading

loadings$Variable <- factor(loadings$Variable, levels = loadings$Variable[order(loadings$MR2, decreasing = T)])

factor2_plot <- ggplot(loadings, aes(x = Variable, y = MR2, fill = MR2 > 0)) +
  geom_col(color="black") +  # Add points for each variable's loading
  labs(title = "2nd Factor Loadings for Each Variable", 
       x = "Variables", 
       y = "2nd Loading") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 8),
        strip.text = element_text(size = 12),  # Increase facet title size
        plot.title = element_text(size = 16),  # Increase overall title size
        axis.title = element_text(size = 12)) +  # Increase axis title size
  scale_color_manual(values = c("red", "blue"), 
                     name = "Loading Sign", 
                     labels = c("Negative", "Positive"))

# 3rd loading

loadings$Variable <- factor(loadings$Variable, levels = loadings$Variable[order(loadings$MR3, decreasing = T)])

factor3_plot <- ggplot(loadings, aes(x = Variable, y = MR3, fill = MR3 > 0)) +
  geom_col(color="black") +  # Add points for each variable's loading
  labs(title = "3rd Factor Loadings for Each Variable", 
       x = "Variables", 
       y = "3rd Loading") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 8),
        strip.text = element_text(size = 12),  # Increase facet title size
        plot.title = element_text(size = 16),  # Increase overall title size
        axis.title = element_text(size = 12)) +  # Increase axis title size
  scale_color_manual(values = c("red", "blue"), 
                     name = "Loading Sign", 
                     labels = c("Negative", "Positive"))

# 4th loading

loadings$Variable <- factor(loadings$Variable, levels = loadings$Variable[order(loadings$MR4, decreasing = T)])

factor4_plot <- ggplot(loadings, aes(x = Variable, y = MR4, fill = MR4 > 0)) +
  geom_col(color="black") +  # Add points for each variable's loading
  labs(title = "4th Factor Loadings for Each Variable", 
       x = "Variables", 
       y = "4th Loading") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 8),
        strip.text = element_text(size = 12),  # Increase facet title size
        plot.title = element_text(size = 16),  # Increase overall title size
        axis.title = element_text(size = 12)) +  # Increase axis title size
  scale_color_manual(values = c("red", "blue"), 
                     name = "Loading Sign", 
                     labels = c("Negative", "Positive"))

# 5th loading

loadings$Variable <- factor(loadings$Variable, levels = loadings$Variable[order(loadings$MR5, decreasing = T)])

factor5_plot <- ggplot(loadings, aes(x = Variable, y = MR5, fill = MR5 > 0)) +
  geom_col(color="black") +  # Add points for each variable's loading
  labs(title = "5th Factor Loadings for Each Variable", 
       x = "Variables", 
       y = "5th Loading") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 8),
        strip.text = element_text(size = 12),  # Increase facet title size
        plot.title = element_text(size = 16),  # Increase overall title size
        axis.title = element_text(size = 12)) +  # Increase axis title size
  scale_color_manual(values = c("red", "blue"), 
                     name = "Loading Sign", 
                     labels = c("Negative", "Positive"))


# 6th loading

loadings$Variable <- factor(loadings$Variable, levels = loadings$Variable[order(loadings$MR6, decreasing = T)])

factor6_plot <- ggplot(loadings, aes(x = Variable, y = MR6, fill = MR6 > 0)) +
  geom_col(color="black") +  # Add points for each variable's loading
  labs(title = "6th Factor Loadings for Each Variable", 
       x = "Variables", 
       y = "6th Loading") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 8),
        strip.text = element_text(size = 12),  # Increase facet title size
        plot.title = element_text(size = 16),  # Increase overall title size
        axis.title = element_text(size = 12)) +  # Increase axis title size
  scale_color_manual(values = c("red", "blue"), 
                     name = "Loading Sign", 
                     labels = c("Negative", "Positive"))

# 7th loading

loadings$Variable <- factor(loadings$Variable, levels = loadings$Variable[order(loadings$MR7, decreasing = T)])

factor7_plot <- ggplot(loadings, aes(x = Variable, y = MR7, fill = MR7 > 0)) +
  geom_col(color="black") +  # Add points for each variable's loading
  labs(title = "7th Factor Loadings for Each Variable", 
       x = "Variables", 
       y = "7th Loading") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 8),
        strip.text = element_text(size = 12),  # Increase facet title size
        plot.title = element_text(size = 16),  # Increase overall title size
        axis.title = element_text(size = 12)) +  # Increase axis title size
  scale_color_manual(values = c("red", "blue"), 
                     name = "Loading Sign", 
                     labels = c("Negative", "Positive"))

# 8th loading

loadings$Variable <- factor(loadings$Variable, levels = loadings$Variable[order(loadings$MR8, decreasing = T)])

factor8_plot <- ggplot(loadings, aes(x = Variable, y = MR8, fill = MR8 > 0)) +
  geom_col(color="black") +  # Add points for each variable's loading
  labs(title = "8th Factor Loadings for Each Variable", 
       x = "Variables", 
       y = "8th Loading") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 8),
        strip.text = element_text(size = 12),  # Increase facet title size
        plot.title = element_text(size = 16),  # Increase overall title size
        axis.title = element_text(size = 12)) +  # Increase axis title size
  scale_color_manual(values = c("red", "blue"), 
                     name = "Loading Sign", 
                     labels = c("Negative", "Positive"))

# Combining plots
factors_plot <- (factor1_plot + factor2_plot) / (factor3_plot + factor4_plot) / (factor5_plot + factor6_plot) / (factor7_plot + factor8_plot)
ggsave("factors_plot.png", plot = factors_plot, width = 14, height = 14, dpi = 300)
```

For the first factor we observe that variables related to passes and game creation are the most influential, so the latent factor could be midfielders or game-creators. For the second factor goalkeeper variables influence it very positively, while variables of defenders, like tackles are very negative, so the latent factor could be goalkeepers (positive) and defenders (negative). For the last factor we see that offensive-game creators, the ones that create possible goal actions, are really high, so the latent factor should be players with this trait.

For the rest, we observe that the data we added from the FIFA dataset seems to have its own factors, like factor 4 that explains high-value players. Moreover, interestingly we observe that the 5th factor only tries to explain older or younger players.

Next, we will see who are the players that score highest and lowest for each factor, to gain more insights about them and confirm what we discussed just before.

```{r}
# Extract factor scores from the players.f object
factor_scores <- players.f$scores

# The current order of factors is 2, 1, 8, 7, 3, 4, 6, 5
current_order <- c(2, 1, 8, 7, 3, 4, 6, 5)

# Loop through each factor to find the largest and smallest observation
num_factors <- ncol(factor_scores)  # Number of factors (columns in factor_scores)

# Create empty lists to store the indices of the largest and smallest observations for each factor
largest_observations <- list()
smallest_observations <- list()

for (i in 1:num_factors) {
  # Get the index of the observation with the largest score for the current factor
  largest_idx <- which.max(factor_scores[, i])
  
  # Get the index of the observation with the smallest score for the current factor
  smallest_idx <- which.min(factor_scores[, i])
  
  # Store the indices in the lists
  largest_observations[[i]] <- largest_idx
  smallest_observations[[i]] <- smallest_idx
}

# Print the results
for (i in 1:num_factors) {
  largest_idx <- largest_observations[[i]]
  smallest_idx <- smallest_observations[[i]]
  largest_name <- full_data$Player[largest_idx]
  smallest_name <- full_data$Player[smallest_idx]
  
  cat("Factor", current_order[i], "\n")
  cat("Largest observation index:", largest_name, "with score:", factor_scores[largest_idx, i], "\n")
  cat("Smallest observation index:", smallest_name, "with score:", factor_scores[smallest_idx, i], "\n\n")
}

```

As we can see with the output, the hidden factors are clear. For example for factor 5 (the "age" factor), the highest one is Joaquin, a player that recently retired, while the lowest one is a player that is now 19. For factor 8, we get Bruno Fernandes, which is an offensive play-maker at Manchester United, just as we explained before. Furthermore, for factor 3 we observe that the highest-scoring player is Haaland, one of the best forwards in the world, and as we look closely at the variables that impact that factor, they are all goal-related.

Finally, we observe that although this analysis does pretty well, it is not perfect, for example in factor 1 (midfielder focused) the highest scoring player is Lewis Dunk an average Premier League midfielder, while Rodri the best player at this places second just behind him.

As a conclusion, we have pretty reasonably performed factor analysis which has given us meaningful hidden factors that could be used from scouting to improve a players performance, or many other possibilities.

# CLUSTERING

# Motivation

As we have discussed before, one of the main goals of the project is to classify players into different groups. One very easy example is well-performing players against bad-performing players, but we could gain even more insights. Due to this clustering is a key-part for this player analysis.

# K-means

# Initial Clustering

Before we move on to fancier methods for k-means, such as kernel-based and choosing thoroughly the number of clusters, we will do a basic initial k-means. The number of cluster will be 4 since at first we think that the player's position could be a very good way to initially describe them.

```{r}
k <- 4
set.seed(1)

initial_kmeans <- eclust(scaled_data, "kmeans", stand=TRUE, k=k)
fviz_silhouette(initial_kmeans)

# Perform PCA on scaled data
pca_result <- prcomp(scaled_data, center = TRUE, scale. = TRUE)

# Extract PCA scores (first two principal components)
pca_cluster_data <- as.data.frame(pca_result$x) %>%
  mutate(Cluster = initial_kmeans$cluster,
         Position = full_data$Pos)  # Assuming 'full_data' has 'Pos' column

# Plot PCA with ggplot2, using color for 'Position' and shape for 'Cluster'
ggplot(pca_cluster_data, aes(x = PC1, y = PC2, color = as.factor(Cluster))) + 
  geom_point(aes(shape = Position, alpha=0.8))+
  scale_shape_manual(values = 1:k) +
  labs(title = "PCA of Clusters with Position Differentiation",
       x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal()
```

As we observe from the silhouette plot, the clusters seem to have a good silhouette score, so the number of clusters may not bee too far away from the optimal ones.

When we plot the clustering against the positions (our initial hypothesis), we observe that the differentiation between them are not that clear. Goalkeepers have their own cluster, as expected, while we see a cluster for maybe more offensive players like midfielders and forwards, and another one for more defensive players, which could be defensive-midfielders or defenders. For the last cluster, it is hard to explain since the position does not seem to matter, therefore it could be classifying average players.

# Deciding number of clusters

```{r}
library("caret")

# Filtering out goalkeepers
full_data_no_gk <- full_data %>% filter(Pos != "GK")
numeric_data_no_gk <- select_if(full_data_no_gk, is.numeric)
# One-hot encode the "Pos" column
dummy <- dummyVars("~ Pos", data = full_data_no_gk)
one_hot_encoded_pos <- predict(dummy, newdata = full_data_no_gk)
# Combine numeric data and the one-hot encoded columns
numeric_data_no_gk <- cbind(numeric_data_no_gk, one_hot_encoded_pos)
numeric_data_no_gk <- numeric_data_no_gk[, !colnames(numeric_data_no_gk) %in% c("GoalsAllowed", "ShotsOnTargetAgainst", "Saves", "Won", "Drawn", "Lost", "CleanSheets")] 
scaled_data_no_gk <- scale(sqrt(numeric_data_no_gk))

one_hot_encoded_pos <- predict(dummy, newdata = full_data)
# Combine numeric data and the one-hot encoded columns
numeric_data <- cbind(numeric_data, one_hot_encoded_pos)
scaled_data <- scale(sqrt(numeric_data))



pca_result <- prcomp(scaled_data)
pca_result_no_gk <- prcomp(scaled_data_no_gk)

# 2. Extract the scores for the first 7 principal components
pca_data <- as.data.frame(pca_result$x[, 1:7])
pca_data_no_gk <- as.data.frame(pca_result_no_gk$x[, 1:7])
```

```{r}
library(cluster)
library(ggplot2)

# Define range of k values
k_values <- 2:10

cluster_comparison <- function(data) {
  # Initialize vectors to store WSS, Silhouette Scores, and Gap Statistic
  wss_values <- numeric(length(k_values))
  silhouette_values <- numeric(length(k_values))
  gap_stat_values <- numeric(length(k_values))

  # Calculate the Gap Statistic
  gap_stat <- clusGap(data, FUN = kmeans, K.max = max(k_values), B = 50)  # B = number of bootstrap samples
  
  for (i in 1:length(k_values)) {
    k <- k_values[i]

    # Fit the k-means model
    kmeans_result <- kmeans(data, centers = k, nstart = 25)

    # Store WSS value (within-cluster sum of squares)
    wss_values[i] <- kmeans_result$tot.withinss

    # Compute the silhouette score
    cluster_assignments <- kmeans_result$cluster
    dist_matrix <- dist(data)  # Euclidean distance matrix
    
    silhouette_score <- silhouette(cluster_assignments, dist_matrix)
    silhouette_values[i] <- mean(silhouette_score[, 3])  # Average silhouette score

    # Store the Gap Statistic value for k clusters
    gap_stat_values[i] <- gap_stat$Tab[i, "gap"]
  }

  # Plot WSS (Elbow Method)
  plot(k_values, wss_values, type = "b", pch = 19, frame = FALSE,
       xlab = "Number of Clusters K",
       ylab = "Within-Cluster Sum of Squares (WSS)",
       main = "Elbow Method for Optimal K")

  # Plot Silhouette Score
  plot(k_values, silhouette_values, type = "b", pch = 19, frame = FALSE,
       xlab = "Number of Clusters K",
       ylab = "Average Silhouette Score",
       main = "Silhouette Method for Optimal K")

  # Plot Gap Statistic
  plot(k_values, gap_stat_values, type = "b", pch = 19, frame = FALSE,
       xlab = "Number of Clusters K",
       ylab = "Gap Statistic",
       main = "Gap Statistic for Optimal K")
}

# Run the function on your datasets
cluster_comparison(scaled_data)
cluster_comparison(scaled_data_no_gk)
cluster_comparison(pca_data)
cluster_comparison(pca_data_no_gk)


```

# Interpretation of clusters

```{r}
#final_kmeans <- kmeans(scaled_data, centers = 5, nstart = 1000)
final_kmeans <- eclust(scaled_data, "kmeans", stand=TRUE, k=5)
fviz_silhouette(final_kmeans)

# Perform PCA on scaled data
pca_result <- prcomp(scaled_data, center = TRUE, scale. = TRUE)


groups <- final_kmeans$cluster
barplot(table(groups), col="blue")
centers <- final_kmeans$centers

barplot(centers[1,], las=2, col="darkblue")
barplot(centers[2,], las=2, col="darkblue")
barplot(centers[3,], las=2, col="darkblue")
barplot(centers[4,], las=2, col="darkblue")
barplot(centers[5,], las=2, col="darkblue")

# Extract PCA scores (first two principal components)
pca_cluster_data <- pca_data %>%
  mutate(Cluster = final_kmeans$cluster,
         Position = full_data$Pos,
         GoalsAssists = full_data$Goals + full_data$Assists,
         CompletedPasses = full_data$CompletedPasses,
         Tackles = full_data$TacklesANDIntercep,
         X90s = full_data$X90s)  # Assuming 'full_data' has 'Pos' column

# Plot PCA with ggplot2, using color for 'Position' and shape for 'Cluster'
ggplot(pca_cluster_data, aes(x = PC1, y = PC2, color = as.factor(Cluster), size = 0.5*GoalsAssists)) + 
  geom_point(aes(shape = Position, alpha=0.8))+
  labs(title = "PCA of Clusters with Position Differentiation",
       x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal()

ggplot(pca_cluster_data, aes(x = PC1, y = PC2, color = as.factor(Cluster), size = 0.5*CompletedPasses)) + 
  geom_point(aes(shape = Position, alpha=0.8))+
  labs(title = "PCA of Clusters with Position Differentiation",
       x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal()

ggplot(pca_cluster_data, aes(x = PC1, y = PC2, color = as.factor(Cluster), size = 0.5*Tackles)) + 
  geom_point(aes(shape = Position, alpha=0.8))+
  labs(title = "PCA of Clusters with Position Differentiation",
       x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal()

ggplot(pca_cluster_data, aes(x = PC1, y = PC2, color = as.factor(Cluster), size = 0.25*X90s)) + 
  geom_point(aes(shape = Position, alpha=0.8))+
  labs(title = "PCA of Clusters with Position Differentiation",
       x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal()
```

```{r}
# Step 1: Create a data frame with cluster assignments and positions
pca_cluster_data <- pca_data %>%
  mutate(Cluster = final_kmeans$cluster, Position = full_data$Pos)

# Step 2: Calculate counts of each Position in each Cluster
position_counts <- pca_cluster_data %>%
  group_by(Cluster, Position) %>%
  summarise(count = n()) %>%
  ungroup()

# Step 3: Calculate the total number of observations in each Cluster
cluster_totals <- pca_cluster_data %>%
  group_by(Cluster) %>%
  summarise(total = n())

# Step 4: Merge the position counts with cluster totals to calculate percentages
position_percentages <- position_counts %>%
  left_join(cluster_totals, by = "Cluster") %>%
  mutate(percentage = (count / total) * 100)

# View the result
position_percentages

ggplot(position_percentages, aes(x = as.factor(Cluster), y = percentage, fill = Position)) +
  geom_bar(stat = "identity", position = "stack", color = "black") +
  labs(title = "Position Percentages in Each Cluster",
       x = "Cluster",
       y = "Percentage") +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) + # Optional: Format y-axis as percentage
  theme_minimal() +
  theme(legend.position = "bottom") +
  guides(fill = guide_legend(title = "Position"))

```

plot_ly(

data = pca_data_3d,

x = \~PC1, \# Replace 'PC1', 'PC2', 'PC3' with actual column names if different

y = \~PC2,

z = \~PC3,

color = \~Cluster,

shape = \~as.factor(Position), \# Color by cluster

text = \~Names, \# Add names as hover text

type = "scatter3d",

mode = "markers", \# Only show markers, not text

marker = list(size = 4, opacity = 0.8),

hoverinfo = "text" \# Show text only on hover

)

```{r}
library(plotly)

# Run PAM clustering
final_pam <- eclust(scaled_data, "pam", stand = TRUE, k = 5, graph = FALSE)

# Extract cluster assignments
cluster_assignments <- factor(final_pam$cluster)

pam_plot <- fviz_cluster(final_pam, data = scaled_data, geom = c("point"), pointsize=1) +
  theme_minimal() +
  geom_text(aes(label = names, color = factor(final_pam$cluster), alpha=0.8), hjust = 0, vjust = 0, size = 2, check_overlap = TRUE) +
  scale_fill_brewer(palette = "Paired") +
  scale_color_brewer(palette = "Paired") # Ensure matching color palette for text

ggsave(filename = "pam_plot.png", plot = pam_plot, width = 10, height = 6, dpi = 300)


# Ensure PCA data is in a data frame format and add cluster and player names
pca_data_3d <- as.data.frame(pca_data) %>%
  mutate(Cluster = cluster_assignments, Names = full_data$Player, Position=full_data$Pos)

plot_ly(
  data = pca_data_3d,
  x = ~PC1,
  y = ~PC2,
  z = ~PC3,
  color = ~Cluster, 
  text = ~paste("Name:", Names, "<br>Position:", Position),
  type = "scatter3d",
  mode = "markers",
  marker = list(size = 4, opacity = 0.8),
  hoverinfo = "text"
)


```

```{r}
fviz_nbclust(scaled_data, pam, method = 'silhouette', k.max = 10)
fviz_nbclust(scaled_data, pam, method = 'gap_stat', k.max = 10, nboot = 10)
adjustedRandIndex(final_kmeans$cluster, final_pam$clustering)
```

## Hierarchical Clustering

Hierarchical Clustering organizes data into a tree type structure. There are different types for it. Following the same explanation as before on PCA, these variables have to be scaled as it is based on distance methods, and therefore different scales could be really costly for the data.

These are: Agglomerative and Divisive Hierarchical Clustering.

On the one hand, agglomerative initally considers each point as a separate cluster, it then computes the distance matrix for the data and joins the two closest points. It repeats this process taking into account the clusters until one cluster with all the points contained is created.

On the other hand, divisive clustering is the opposite of the previous. It considers all data points together on one single cluster, it then divides into smaller clusters, until all points form a singular cluster. It will do this obtaining the two most dissimilar elements in a cluster, and separating them. Computer wise this is a bit more complex as it is harder to find the optimal splits rather than the two closes points.

First of all we will obtain the distances as well as the linkage. As there is no single formula for which distance or linkage method is better, we will plot the graphs using different combinations of a few of them explained below and analyze the results to see which method is better.

For computing the distance matrix there are many different methods such as Euclidean, given by this formula:

$$ d(i, j) = \sqrt{\sum_{k=1}^{n} \left( x_{ik} - x_{jk} \right)^2} $$

We also have Canberra distance given by:

$$
d(i, j) = \sum_{k=1}^{n} \frac{\left| x_{ik} - x_{jk} \right|}{\left| x_{ik} \right| + \left| x_{jk} \right|}
$$

There are many more methods such as Manhattan or Minkowsky, but for this project we just want to try a few methods and see which one is better.

There are 4 main linkage methods, which we will test to see which one produces the most interesting result. Linkage methods are used to measure the distance between two clusters taking into account the points inside the clusters

We have simple linkage, which measures the shortest distance:

$$
d(C_1, C_2) = \min \left\{ d(x, y) \mid x \in C_1, y \in C_2 \right\}
$$

We have average linkage, measures average distance:

$$
d(C_1, C_2) = \frac{1}{|C_1| |C_2|} \sum_{x \in C_1, y \in C_2} d(x, y)
$$

We also have complete linkage, which measures the furthest distance

$$
d(C_1, C_2) = \max \left\{ d(x, y) \mid x \in C_1, y \in C_2 \right\}
$$

And finally we have Ward\`s method which focuses on minimizing variance

$$
d(C_1, C_2) = \sqrt{\frac{|C_1| |C_2|}{|C_1| + |C_2|} \left( \mu_{C_1} - \mu_{C_2} \right)^T \left( \mu_{C_1} - \mu_{C_2} \right)}
$$

These is how we computed the different distances and plotted the different graphs

```{r}
?stats::dist
?hclust
d1 = dist(scaled_data, method = "euclidean")
d2 = dist(scaled_data, method = "canberra")



hc_euclidean_complete = hclust(d1, method = "complete")
hc_euclidean_single = hclust(d1, method = "single")
hc_euclidean_average = hclust(d1, method = "average")
hc_euclidean_ward = hclust(d1, method = "ward.D")

hc_canberra_complete = hclust(d2, method = "complete")
hc_canberra_single = hclust(d2, method = "single")
hc_canberra_average = hclust(d2, method = "average")
hc_canberra_ward = hclust(d2, method = "ward.D")

```

To see which method is better we will obtain the silhouette values for each of the hierarchical clusters we have done before. The one who yields a mean value closes to one will be the best one. As this one will indicate that the point is closest to its own cluster rather than the nearest other cluster. If the value is negative this will likely be a misclassification.

Based on our previous clustering analysis we will use the cutree function to cut at 5 clusters, which as seen in the previous method, it appears to be the best number of clusters for our data.

```{r}
# Establish the number of clusters where to cut the dendogram
chosen_k = 5

# Calculate Silhouette Scores for Euclidean Distance
silhouette_ec <- silhouette(cutree(hc_euclidean_complete, k = chosen_k), d1)
mean_silhouette_ec <- mean(silhouette_ec[, 3])

silhouette_es <- silhouette(cutree(hc_euclidean_single, k = chosen_k), d1)
mean_silhouette_es <- mean(silhouette_es[, 3])

silhouette_ea <- silhouette(cutree(hc_euclidean_average, k = chosen_k), d1)
mean_silhouette_ea <- mean(silhouette_ea[, 3])

silhouette_ew <- silhouette(cutree(hc_euclidean_ward, k = chosen_k), d1)
mean_silhouette_ew <- mean(silhouette_ew[, 3])

# Calculate Silhouette Scores for Canberra Distance
silhouette_cc <- silhouette(cutree(hc_canberra_complete, k = chosen_k), d2)
mean_silhouette_cc <- mean(silhouette_cc[, 3])

silhouette_cs <- silhouette(cutree(hc_canberra_single, k = chosen_k), d2)
mean_silhouette_cs <- mean(silhouette_cs[, 3])

silhouette_ca <- silhouette(cutree(hc_canberra_average, k = chosen_k), d2)
mean_silhouette_ca <- mean(silhouette_ca[, 3])

silhouette_cw <- silhouette(cutree(hc_canberra_ward, k = chosen_k), d2)
mean_silhouette_cw <- mean(silhouette_cw[, 3])

mean_silhouettes <- c(
  "Euclidean Complete" = mean_silhouette_ec,
  "Euclidean Single" = mean_silhouette_es,
  "Euclidean Average" = mean_silhouette_ea,
  "Euclidean Ward" = mean_silhouette_ew,
  "Canberra Complete" = mean_silhouette_cc,
  "Canberra Single" = mean_silhouette_cs,
  "Canberra Average" = mean_silhouette_ca,
  "Canberra Ward" = mean_silhouette_cw
)

best_method <- names(mean_silhouettes)[which.max(mean_silhouettes)]
best_value <- max(mean_silhouettes)

print(paste("Best method is:", best_method, "with a value of ", best_value))


```

From all the methods obtained, we see the best obtained was with Euclidean Distance and single linkage.

d1 = dist(scaled_data, method = "euclidean")

hc_euclidean_single = hclust(d1, method = "single")

```{r}
fviz_dend(x = hc_euclidean_single,
          k=5,
          palette = "jco", 
          rect = TRUE, rect_fill = TRUE, 
          rect_border = "jco"          
)


```

As it will be complicated to see this using a dendogram, we will do it using a phylogenic tree.

```{r}
fviz_dend(x = hc_euclidean_single,
          k = 5,
          color_labels_by_k = TRUE,
          cex = 0.8,
          type = "phylogenic",
          repel = TRUE)+  labs(title="Top 5 European Football Leagues Tree Structure") + theme(axis.text.x=element_blank(),axis.text.y=element_blank())
```

## EM CLUSTERING

Assumes each data variable follows a probability distribution, in our case Gaussian Distribution, calculates the probability of each point belonging to each cluster based on the current estimates. It will update the parameters such as covariances to maximize the likelihood of the data

```{r}
res.Mclust <- Mclust(scaled_data)
summary(res.Mclust)
```

This returns that all points belong to one cluster.

## Heatmaps

Heatmaps are fantastic ways to see the relationships of data, it will use dendograms, which are structures already seen in hierarchical clustering.

This will be done, with Euclidean and Single method as we have seen this is the best method.

To plot this heatmap, we will use the pheatmap library as with the number of observations we currently have

```{r}
library(pheatmap)

# Generate heatmap with better control over dendrogram appearance
pheatmap(scaled_data,
         clustering_distance_rows = "euclidean",
         clustering_method = "single",
         fontsize_row = 8,   
         fontsize_col = 8,   
         angle_col = 90,      
         treeheight_row = 50, 
         treeheight_col = 50) 
```

These graph is not really good for visualizing the similarities between players as there are around 2000 players for which the distances are computed and it is really hard to visualize this. But it shows the similarities between many variables so this graph is useful for our project. We can see by looking at the length of the branches, the shortest the branch the closer two variables are related. By looking at the graph we can see that these relations make sense, such as Shots On Target Against and Saves which are really related. Attempted Passes and Touches are also really similar, these are just a few examples, but both of these combinations make a lot of sense.
